[
    {
		"title": "A Late Fusion CNN for Digital Matting",
		"authors": [
            {
				"fullName": "Yunke Zhang",
				"url": "",
				"afflID": [0]
			},
			{
				"fullName": "Lixue Gong",
				"url": "",
				"afflID": [0]
			},
            {
				"fullName": "Lubin Fan",
				"url": "/",
				"afflID": [1]
			},
            {
				"fullName": "Peiran Ren",
				"url": "",
				"afflID": [1]
			},
            {
				"fullName": "Qixing Huang",
				"url": "http://www.cs.utexas.edu/~huangqx/",
				"afflID": [2]
			},
			{
				"fullName": "Hujun Bao",
				"url": "http://www.cad.zju.edu.cn/home/bao/",
				"afflID": [0]
			},
			{
				"fullName": "Weiwei Xu",
				"url": "http://www.cad.zju.edu.cn/home/weiweixu/weiweixu_en.htm",
				"afflID": [0]
			}
		],
		"affiliations": [
            {
				"name": "Zhejiang University",
				"url": "http://www.zju.edu.cn/"
			},
            {
				"name": "Alibaba Group",
				"url": "https://www.alibabagroup.com/en/global/home"
			},
			{
				"name": "University of Texas at Austin",
				"url": "https://www.utexas.edu/"
			}
		],
		"event": "CVPR 2019",
		"abstract": "This paper studies the structure of a deep convolutional neural network to predict the foreground alpha matte by taking a single RGB image as input. Our network is fully convolutional with two decoder branches for the foreground and background classification respectively. Then a fusion branch is used to integrate the two classification results which gives rise to alpha values as the soft segmentation result. This design provides more degrees of freedom than a single decoder branch for the network to obtain better alpha values during training. The network can implicitly produce trimaps without user interaction, which is easy to use for novices without expertise in digital matting. Experimental results demonstrate that our network can achieve high-quality alpha mattes for various types of objects and outperform the state-of-the-art CNN-based image matting methods on the human image matting task.",
		"contents": [{
				"name": "",
				"images": [{
					"type":"Figure 1",
					"uri": "images/publications/2019-cvpr-matting/teaser.png",
					"caption": "Two example matting results of our late fusion CNN that does not need trimaps as input. Left: two images collected from internet outside of our training dataset. Middle: the alpha mattes predicted by our network. Right: the composition results."
				}]
			},
			{
				"name": "Network Architecture",
				"images": [{
					"type":"Figure 2",
					"uri": "images/publications/2019-cvpr-matting/network.png",
					"caption": "A high-level visualization of our network architecture. The segmentation network consists of one encoder and two decoders. The fusion network is a fully convolutional network without downsampling. The final alpha matte is a linear blending using the outputs of two networks. The number below the block in the fusion network denotes the number of output channels of different convolution layers."
				}]
			},
			{
				"name": "Results",
				"images": [{
					    "type":"Figure 3",
						"uri": "images/publications/2019-cvpr-matting/results.png",
						"caption": "Internet image matting results."
					},
					{
						"type":"Figure 4",
						"uri": "images/publications/2019-cvpr-matting/comparison.png",
						"caption": "The visual comparisons on human image matting testing dataset. The segments in SSS [2] are hand-picked."
					}
				]
			}
		],
		"videos": [
		
		],
		"acknowlegement": "We would like to thank the anonymous reviewers for their constructive comments. Weiwei Xu is partially supported by NSFC (No. 61732016) and Zhejiang Lab. Qixing Huang would like to thank for the gift from snap research. Weiwei Xu and Hujun Bao are also supported by the Fundamental Research Funds for the Central Universities.",
		"bibtex": {
			"type": "article",
			"name": "Zhang:2019:Matting",
			"title": "A Late Fusion CNN for Digital Matting",
			"author": "Yunke Zhang and Lixue Gong and Lubin Fan and Peiran Ren and Qixing Huang and Hujun Bao and Weiwei Xu",
			"journel": "CVPR",
			"volume": "",
			"number": "",
			"pages": "",
			"year": "2019"
		},
		"materials": [{
				"iconUri": "images/icons/pdf_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2019-cvpr-matting.pdf",
				"caption": "Paper"
			},
			{
				"iconUri": "images/icons/pdf_wh64.png",
				"matUri": "",
				"caption": "Additional Materials"
			}
		]
	},
	{
		"title": "MIQP-based Layout Design for Building Interiors",
		"authors": [{
				"fullName": "Wenming Wu",
				"url": "/",
				"afflID": [0, 1]
			},
			{
				"fullName": "Lubin Fan",
				"url": "/",
				"afflID": [0]
			},
			{
				"fullName": "Ligang Liu",
				"url": "http://staff.ustc.edu.cn/~lgliu/",
				"afflID": [1]
			},
			{
				"fullName": "Peter Wonka",
				"url": "http://peterwonka.net/",
				"afflID": [0]
			}
		],
		"affiliations": [{
				"name": "King Abdullah University of Science and Technology",
				"url": "http://www.kaust.edu.sa/index.html"
			},
			{
				"name": "University of Science and Technology of China",
				"url": "http://en.ustc.edu.cn/"
			}
		],
		"event": "Computer Grahpics Forum (Proc. of Eurographics 2018)",
		"abstract": "We propose a hierarchical framework for the generation of building interiors. Our solution is based on a mixed integer quadratic programming (MIQP) formulation. We parametrize a layout by polygons that are further decomposed into small rectangles. We identify important high-level constraints, such as room size, room position, room adjacency, and the outline of the building, and formulate them in a way that is compatible with MIQP and the problem parametrization. We also propose a hierarchical framework to improve the scalability of the approach. We demonstrate that our algorithm can be used for residential building layouts and can be scaled up to large layouts such as office buildings, shopping malls, and supermarkets. We show that our method is faster by multiple orders of magnitude than previous methods.",
		"contents": [{
				"name": "",
				"images": [{
					"type":"Figure 1",
					"uri": "images/publications/2018-eg-iplayout/teaser.png",
					"caption": "We propose a framework that generates building interiors with high-level constraints, e.g., room size, room position, room adjacency, and the outline of the building. (a) The layout of a two-storey house and corresponding 3D renderings. (b) A large-scale example depicting an office building. For such large-scale layouts, our method is faster by multiple orders of magnitude than previous methods."
				}]
			},
			{
				"name": "Overview",
				"images": [{
					"type":"Figure 2",
					"uri": "images/publications/2018-eg-iplayout/overview.png",
					"caption": "Overview of our framework. Given a list of high-level constraints and an outline as input (Input), we compute a layout using a hierarchical framework. Level 1 shows an initial layout and Level 2 shows the layout including local refinements. Finally, additional details can be added to visualize the results as architectural floorplans or 3D models."
				}]
			},
			{
				"name": "Results",
				"images": [{
					    "type":"Figure 3",
						"uri": "images/publications/2018-eg-iplayout/results.png",
						"caption": "Two apartment layouts are generated by our algorithm with the same input. Boundary constraints are added for bedrooms to ensure that all bedrooms receive sufficient sunshine from the south east direction. The north direction is shown."
					},
					{ 
						"type":"Figure 4",
						"uri": "images/publications/2018-eg-iplayout/hierarchical.png",
						"caption": "Pipeline of the hierarchical framework. (a) Starting from the empty layout domain with the door position and two obstacles, our method generates (b) an interior layout on the first level according to the user-specified high-level constraints. There is a region (shown in red) that is not covered by any room. (c) Our algorithm automatically selects the sub-domain (highlighted as the red polygon) for the optimization on the next level. (d) Then, the sub-domain is initialized with small rectangles and constraints are updated. (e) The new layout is generated in the sub-domain (i.e.,red polygon). The user specifies the study room (blue rectangle) for further improvement. (f) The final result."
					},
					{
						"type":"Figure 5",
						"uri": "images/publications/2018-eg-iplayout/mall.png",
						"caption": "An interior layout of a shopping mall is generated by our hierarchical framework. (a) Starting from an empty layout domain with obstacles. (b) First, we generate the layouts for five regions shown in different colors. (c) Then aisles are generated by expanding the gap between two regions with a fixed width. (d) Next, we generate the layouts inside each region. (e-h) We repeat this procedure to get (i) the final layout. The number of objects (regions and rooms) and constraints on each level are specified by the user."
					}
				]
			}
		],
		"videos": [
		{
			"imageUri":"",
			"caption": [
			{
				"title":"Youku",
				"uri":""
			},
			{
				"title":"YouTube",
				"uri":""
			}
			]
		}
		],
		"acknowlegement": "This work was supported by the KAUST Office of Sponsored Research (OSR) under Award No. OCRF-2014-CGR3-62140401, and the Visual Computing Center at KAUST. Ligang Liu is supported by the National Natural Science Foundation of China (61672482, 61672481, 11626253) and the One Hundred Talent Project of the Chinese Academy of Sciences. We would like to thank Virginia Unkefer for proofreading the paper.",
		"bibtex": {
			"type": "article",
			"name": "Fan:2018:IPLayout",
			"title": "MIQP-based Layout Design for Building Interiors",
			"author": "Wenming Wu and Lubin Fan and Ligang Liu and Peter Wonka",
			"journel": "Computer Grahpics Forum",
			"volume": "37",
			"number": "2",
			"pages": "511--521",
			"year": "2018"
		},
		"materials": [{
				"iconUri": "images/icons/pdf_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2018-eg-iplayout.hres.pdf",
				"caption": "Paper (108.1MB)"
			},
			{
				"iconUri": "images/icons/pdf_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2018-eg-iplayout.suppl.pdf",
				"caption": "Additional Materials (3.37MB)"
			},
			{
				"iconUri": "images/icons/video_wh64.png",
				"matUri": "",
				"caption": "Video"
			}
		]
	},
	{
		"title": "A Probabilistic Model for Exteriors of Residential Buildings",
		"authors": [{
				"fullName": "Lubin Fan",
				"url": "/",
				"afflID": [0]
			},
			{
				"fullName": "Peter Wonka",
				"url": "http://peterwonka.net/",
				"afflID": [0]
			}
		],
		"affiliations": [{
			"name": "King Abdullah University of Science and Technology",
			"url": "http://www.kaust.edu.sa/index.html"
		}],
		"event": "ACM Transactions on Graphics 2016",
		"abstract": "We propose a new framework to model the exterior of residential buildings. The main goal of our work is to design a model that can be learned from data that is observable from the outside of a building and that can be trained with widely available data such as aerial images and street view images. First, we propose a parametric model to describe the exterior of a building (with a varying number of parameters) and propose a set of attributes as a building representation with fixed dimensionality. Second, we propose a hierarchical graphical model with hidden variables to encode the relationships between building attributes and learn both the structure and parameters of the model from the database. Third, we propose optimization algorithms to generate three-dimensional models based on building attributes sampled from the graphical model. Finally, we demonstrate our framework by synthesizing new building models and completing partially observed building models from photographs. ",
		"contents": [{
				"name": "",
				"images": [{
					"type":"Figure 1",
					"uri": "images/publications/2016-tog-bldg_model/teaser.png",
					"caption": "An application of our building model. Starting from a single image (Input), a user can specify parts of a building mass model and mark shapes (windows and doors) on the observed part of a building facade in a rectified image (Element Selection). Our framework can complete the missing parts of the building (mass model and facades). A 3D rendering of the completed building is shown on the bottom."
				}]
			},
			{
				"name": "Overview",
				"images": [{
					"type":"Figure 2",
					"uri": "images/publications/2016-tog-bldg_model/overview.png",
					"caption": "Overview of our framework."
				}]
			},
			{
				"name": "Results",
				"images": [{
					   "type":"Figure 3",
						"uri": "images/publications/2016-tog-bldg_model/synthesis.png",
						"caption": "Six buildings synthesized using our algorithm."
					},
					{
						"type":"Figure 4",
						"uri": "images/publications/2016-tog-bldg_model/completion.png",
						"caption": "Given an incomplete building model, our algorithm can generate multiple completions. Top row: photograph of the building, and two views of the incomplete building. Middle and bottom rows show three possible completions. For each completion, the front and back of the building are shown."
					}
				]
			}
		],
		"videos": [
		{
			"imageUri":"images/publications/2016-tog-bldg_model/video.png",
			"caption": [
			{
				"title":"Youku",
				"uri":"https://v.youku.com/v_show/id_XNDE3MTEwMjE4OA==.html?spm=a2h3j.8428770.3416059.1"
			},
			{
				"title":"YouTube",
				"uri":"https://www.youtube.com/watch?v=ao0b28bk6cQ"
			}
			]
		}
		],
		"acknowlegement": "We would like to acknowledge the help of Yoshihiro Kobayashi and Christopher Grasso for rendering most of the images in this paper, the help of Tina Smith for video narration, and the help of Virginia Unkefer for proofreading. This publication is based upon work supported by the Office of Sponsored Research (OSR) under Award No. OCRF-2014-CRG3-62140401 and the KAUST Visual Computing Center.",
		"bibtex": {
			"type": "article",
			"name": "Fan:2016:BldgModel",
			"title": "A Probabilistic Model for Exteriors of Residential Buildings",
			"author": "Lubin Fan and Peter Wonka",
			"journel": "ACM Transactions on Graphics",
			"volume": "35",
			"number": "5",
			"pages": "155:1--155:13",
			"year": "2016"
		},
		"materials": [{
				"iconUri": "images/icons/pdf_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2016-tog-bldg_model.pdf",
				"caption": "Paper (2.18MB)"
			},
			{
				"iconUri": "images/icons/pdf_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2016-tog-bldg_model.suppl.pdf",
				"caption": "Additional Materials (1.71MB)"
			},
			{
				"iconUri": "images/icons/zip_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2016-tog-bldg_model.data.pdf",
				"caption": "Dataset (188MB)"
			},
			{
				"iconUri": "images/icons/video_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2016-tog-bldg_model.mp4",
				"caption": "Video  (39.5MB)"
			},
			{
				"iconUri": "images/icons/ppt_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2016-tog-bldg_model.slides.pdf",
				"caption": "Slides (5.58MB)"
			}
		]
	},
	{
		"title": "Structure Completion for Facade Layouts",
		"authors": [{
				"fullName": "Lubin Fan",
				"url": "/",
				"afflID": [0, 1]
			},
			{
				"fullName": "Przemyslaw Musialski",
				"url": "http://www.cg.tuwien.ac.at/~pm/",
				"afflID": [2]
			},
			{
				"fullName": "Ligang Liu",
				"url": "http://staff.ustc.edu.cn/~lgliu/",
				"afflID": [3]
			},
			{
				"fullName": "Peter Wonka",
				"url": "http://peterwonka.net/",
				"afflID": [0, 4]
			}
		],
		"affiliations": [{
				"name": "King Abdullah University of Science and Technology",
				"url": "http://www.kaust.edu.sa/index.html"
			},
			{
				"name": "Zhejiang University",
				"url": "http://www.zju.edu.cn/english/"
			},
			{
				"name": "Vienna University of Technology",
				"url": "http://www.tuwien.ac.at/"
			},
			{
				"name": "University of Science and Technology of China",
				"url": "http://en.ustc.edu.cn/"
			},
			{
				"name": "Arizona State University",
				"url": "http://en.ustc.edu.cn/"
			}
		],
		"event": "ACM Transactions on Graphics (Proc. of SIGGRAPH Asia), 2014",
		"abstract": "We present a method to complete missing structures in facade layouts.Starting from an abstraction of the partially observed layout as a set of shapes, we can propose one or multiple possible completed layouts. Structure completion with large missing parts is an ill-posed problem. Therefore, we combine two sources of information to derive our solution: the observed shapes and a database of complete layouts. The problem is also very difficult, because shape positions and attributes have to be estimated jointly. Our proposed solution is to break the problem into two components: a statistical model to evaluate layouts and a planning algorithm to generate candidate layouts. This ensures that the completed result is consistent with the observation and the layouts in the database.",
		"contents": [{
				"name": "",
				"images": [{
					"type":"Figure 1",
					"uri": "images/publications/2014-sga-facade_model/teaser.png",
					"caption": "Starting from a single image (Input) a user can specify a building mass model and mark shapes (windows, doors, ornaments) on the observed part of a building facade in a rectified image (Element Selection). Our structure completion framework can complete the missing part of the layout (Structure Completion). A 2D and a 3D rendering of the completed models is shown on the right."
				}]
			},
			{
				"name": "Results",
				"images": [{
					    "type":"Figure 2",
						"uri": "images/publications/2014-sga-facade_model/strasbourg.png",
						"caption": "Completion of a series of buildings along a street occluded by trees."
					},
					{
						"type":"Figure 3",
						"uri": "images/publications/2014-sga-facade_model/gallery.png",
						"caption": "Several 3D models created using completion results."
					}
				]
			}
		],
		"videos": [
		{
			"imageUri":"images/publications/2014-sga-facade_model/video.png",
			"caption": [
			{
				"title":"Youku",
				"uri":"https://v.youku.com/v_show/id_XNDE4NzM1Mjk1Mg==.html?spm=a2h3j.8428770.3416059.1"
			},
			{
				"title":"YouTube",
				"uri":"https://www.youtube.com/watch?v=ISWwk82uP0A"
			}
			]
		}
		],
		"acknowlegement": "We would like to thank all the reviewers for their constructive comments. This research was partially funded by the Visual Computing Center of King Abdullah University of Science and Technology (KAUST), the National Natural Science Foundation of China (No. 61222206), the One Hundred Talent Project of the Chinese Academy of Sciences, the U.S. National Science Foundation (CCF 0643822), and the Austrian Science Funds (FWF P24600-N23).",
		"bibtex": {
			"type": "article",
			"name": "Fan:2014:FacadeModel",
			"title": "Structure Completion for Facade Layouts",
			"author": "Lubin Fan and Przemyslaw Musialski and Ligang Liu and Peter Wonka",
			"journel": "ACM Trans. Graph. (Proc. of SIGGRAPH Asia 2014)",
			"volume": "33",
			"number": "6",
			"pages": "210:1--210:1",
			"year": "2014"
		},
		"materials": [{
				"iconUri": "images/icons/pdf_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2014-sga-facade_model.hres.pdf",
				"caption": "Paper (61.9MB)"
			},
			{
				"iconUri": "images/icons/pdf_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2014-sga-facade_model.lres.pdf",
				"caption": "Paper (5.4MB)"
			},
			{
				"iconUri": "images/icons/pdf_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2014-sga-facade_model.suppl.pdf",
				"caption": "Additional Materials (5.8MB)"
			},
			{
				"iconUri": "images/icons/zip_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2014-sga-facade_model.data.zip",
				"caption": "Dataset (173.0MB)"
			},
			{
				"iconUri": "images/icons/video_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2014-sga-facade_model.video1.mp4",
				"caption": "Video 1 (66.7MB)"
			},
			{
				"iconUri": "images/icons/video_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2014-sga-facade_model.video2.mp4",
				"caption": "Video 2 (58.3MB)"
			},
			{
				"iconUri": "images/icons/ppt_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2014-sga-facade_model.slides.pdf",
				"caption": "Slides (102MB)"
			}
		]
	},
	{
		"title": "Modeling by Drawing with Shadow Guidance",
		"authors": [{
				"fullName": "Lubin Fan",
				"url": "/",
				"afflID": [0]
			},
			{
				"fullName": "Ruiming Wang",
				"url": " ",
				"afflID": [1]
			},
			{
				"fullName": "Linlin Xu",
				"url": " ",
				"afflID": [1]
			},
			{
				"fullName": "Jiansong Deng",
				"url": "http://staff.ustc.edu.cn/~dengjs/",
				"afflID": [1]
			},
			{
				"fullName": "Ligang Liu",
				"url": "http://staff.ustc.edu.cn/~lgliu/",
				"afflID": [1]
			}
		],
		"affiliations": [{
				"name": "Zhejiang University",
				"url": "http://www.zju.edu.cn/"
			},
			{
				"name": "University of Science and Technology of China",
				"url": "http://en.ustc.edu.cn/"
			}
		],
		"event": "Computer Graphics Forum (Proc. of Pacific Graphics), 2013",
		"abstract": "Modeling 3D objects is difficult, especially for the user who lacks the knowledge on 3D geometry or even on 2D sketching. In this paper, we present a novel sketch-based modeling system which allows novice users to create 3D custom models by assembling parts based on a database of pre-segmented 3D models. Different from previous systems, our system supports the user with visualized and meaningful shadow guidance under his strokes dynamically to guide the user to convey his design concept easily and quickly. Our system interprets the user’s strokes as similarity queries into database to generate the shadow image for guiding the user’s further drawing and returns the 3D candidate parts for modeling simultaneously. Moreover, our system preserves the high-level structure in generated models based on prior knowledge pre-analyzed from the database, and allows the user to create custom parts with geometric variations. We demonstrate the applicability and effectiveness of our modeling system with human subjects and present various models designed using our system. ",
		"contents": [{
				"name": "",
				"images": [{
					"type":"Figure 1",
					"uri": "images/publications/2013-pg-sketch_modeling/teaser.png",
					"caption": "An example of creating a chair model using our system. The user successively draws freeform strokes (red) to express his design concept on the screen. Our system automatically returns the corresponding 3D parts and dynamically provides shadow guidance to the user for further drawing. (a)-(e) shows strokes specified by the user, shadow provided by our system (the upper row) and the corresponding 3D models generated in our system (the lower row). The user can also edit the part by drawing near the silhouette of the model (e). (f) shows the final model generated by our system."
				}]
			},
			{
				"name": "System Pipeline",
				"images": [{
					"type":"Figure 2",
					"uri": "images/publications/2013-pg-sketch_modeling/pipeline.png",
					"caption": "System pipeline. The user can construct a 3D model in a part-by-part fashion. As the user draws on the canvas, our system interprets user's strokes as similarity queries into database to compose the shadow image and return the corresponding candidate 3D parts. Then the system assembles the selected part by taking user's strokes and prior knowledge as hints. The user can draw strokes near the added parts to make variations."
				}]
			},
			{
				"name": "Results",
				"images": [{
					"type":"Figure 3",
					"uri": "images/publications/2013-pg-sketch_modeling/gallery.png",
					"caption": "3D models created by users using our system."
				}]
			}
		],
		"videos": [
		{
			"imageUri":"",
			"caption": [
			{
				"title":"Youku",
				"uri":"https://v.youku.com/v_show/id_XNDE4NzM1NDA0MA==.html?spm=a2h3j.8428770.3416059.1"
			},
			{
				"title":"YouTube",
				"uri":""
			}
			]
		}
		],
		"acknowlegement": "We would like to thank all the reviewers for their constructive comments. This work is supported by the National Natural Science Foundation of China (61222206, 61073108, 11031007) and the One Hundred Talent Project of the Chinese Academy of Sciences.",
		"bibtex": {
			"type": "article",
			"name": "Fan:2013:SketchModeling",
			"title": "Modeling by Drawing with Shadow Guidance",
			"author": "Lubin Fan and Ruimin Wang and Linlin Xu and Jiansong Deng and Ligang Liu",
			"journel": "Computer Graphics Forum (Proc. of Pacific Graphics 2013)",
			"volume": "23",
			"number": "7",
			"pages": "157-166",
			"year": "2013"
		},
		"materials": [{
				"iconUri": "images/icons/pdf_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2013-pg-sketch_modeling.pdf",
				"caption": "Paper (5.0MB)"
			},
			{
				"iconUri": "images/icons/zip_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2013-pg-sketch_modeling.suppl.pdf",
				"caption": "Supplementary Materials (11.0MB)"
			},
			{
				"iconUri": "images/icons/video_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2013-pg-sketch_modeling.mov",
				"caption": "Video (29.8MB)"
			},
			{
				"iconUri": "images/icons/ppt_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2013-pg-sketch_modeling.slides.pdf",
				"caption": "Slides (51.2MB)"
			}
		]
	},
	{
		"title": "Co-Segmentation of 3D Shapes via Subspace Clustering",
		"authors": [{
				"fullName": "Ruizhen Hu",
				"url": "http://csse.szu.edu.cn/staff/ruizhenhu/",
				"afflID": [0]
			},
			{
				"fullName": "Lubin Fan",
				"url": "/",
				"afflID": [0]
			},
			{
				"fullName": "Ligang Liu",
				"url": "http://staff.ustc.edu.cn/~lgliu/",
				"afflID": [0]
			}
		],
		"affiliations": [{
			"name": "Zhejiang University",
			"url": "http://www.zju.edu.cn/english/"
		}],
		"event": "Computer Graphics Forum (Proc. of Eurographics Symposium on Geometry Processing), 2012",
		"abstract": "We present a novel algorithm for automatically co-segmenting a set of shapes from a common family into consistent parts. Starting from over-segmentations of shapes, our approach generates the segmentations by grouping the primitive patches of the shapes directly and obtains their correspondences simultaneously. The core of the algorithm is to compute an affinity matrix where each entry encodes the similarity between two patches, which is measured based on the geometric features of patches. Instead of concatenating the different features into one feature descriptor, we formulate co-segmentation into a subspace clustering problem in multiple feature spaces. Specifically, to fuse multiple features, we propose a new formulation of optimization with a consistent penalty, which facilitates both the identification of most similar patches and selection of master features for two similar patches. Therefore the affinity matrices for various features are sparsity-consistent and the similarity between a pair of patches may be determined by part of (instead of all) features. Experimental results have shown how our algorithm jointly extracts consistent parts across the collection in a good manner.",
		"contents": [{
				"name": "",
				"images": [{
					"type":"Figure 1",
					"uri": "images/publications/2012-sgp-coseg/teaser.png",
					"caption": "The co-segmentation result of the Candelabra set by our algorithm. Starting from the over-segmented patches of the shapes (left), our algorithm automatically obtains the consistent segmentations among these objects by grouping the patches using subspace clustering in multiple feature spaces. Corresponding parts are shown in the same colors (right)."
				}]
			},
			{
				"name": "Results",
				"images": [{
					"type":"Figure 2",
					"uri": "images/publications/2012-sgp-coseg/results.png",
					"caption": " Co-segmentation results on all the models of 20 categories produced by our algorithm. 16 categories are from the PSB dataset [CGF09] (the first 4 rows) and 4 categories are from the dataset used in [SvKK 11] (the last row)."
				}]
			}
		],
		"videos": [
		{
			"imageUri":"",
			"caption": [
			{
				"title":"Youku",
				"uri":"https://v.youku.com/v_show/id_XNDE4NzM1NDI1Ng==.html?spm=a2h3j.8428770.3416059.1"
			},
			{
				"title":"YouTube",
				"uri":""
			}
			]
		}
		],
		"acknowlegement": "We would like to thank all the reviewers for their constructive comments. We thank Oliver van Kaick and Richard Zhang for their results and dataset of [SvKK*11], Evangelos Kalogerakis and Aaron Hertzmann for their results and dataset of [KHS10], Huang Qixing for his code of oversegmentation, Shusen Wang for his code of SSQP, and Juan Sagredo for video narration. This work is supported by the National Natural Science Foundation of China (61070071) and the 973 National Key Basic Research Foundation of China (2009CB320801).",
		"bibtex": {
			"type": "article",
			"name": "Hu:2012:CoSeg",
			"title": "Co-Segmentation of 3D Shapes via Subspace Clustering",
			"author": "Ruizhen Hu and Lubin Fan and Ligang Liu",
			"journel": "Computer Graphics Forum (Proc. of SGP 2012)",
			"volume": "31",
			"number": "5",
			"pages": "1703-1713",
			"year": "2012"
		},
		"materials": [{
				"iconUri": "images/icons/pdf_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2012-sgp-coseg.pdf",
				"caption": "Paper (1.3MB)"
			}, {
				"iconUri": "images/icons/pdf_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2012-sgp-coseg.suppl.pdf",
				"caption": "Supplementary Material (0.2MB)"
			},

			{
				"iconUri": "images/icons/video_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2012-sgp-coseg.mov",
				"caption": "Video (46.6MB)"
			},
			{
				"iconUri": "images/icons/ppt_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2012-sgp-coseg.slides.pdf",
				"caption": "Slides (9.4MB)"
			}
		]
	},
	{
		"title": "Sketch-based Mesh Cutting: A Comparative Study",
		"authors": [{
				"fullName": "Lubin Fan",
				"url": "/",
				"afflID": [0]
			},
			{
				"fullName": "Min Meng",
				"url": "",
				"afflID": [0]
			},
			{
				"fullName": "Ligang Liu",
				"url": "http://staff.ustc.edu.cn/~lgliu/",
				"afflID": [0]
			}
		],
		"affiliations": [{
			"name": "Zhejiang University",
			"url": "http://www.zju.edu.cn/english/"
		}],
		"event": "Grphical Models, 74(6): 292-301, 2012",
		"abstract": "In this paper we present the first comprehensive study and analysis on different sketch-based mesh cutting approaches. To compare a representative number of state-of-the-art sketch-based mesh cutting methods, we conduct a large scale user study which was carried out via extensive user experiments. To address the objective assessment of the performances of different algorithms, a complete framework with various intuitive sketching interfaces was developed to enable interactive mesh cutting by simply drawing sketches on mesh surface. To address the subjective assessment of user's experience, we presented the analysis of the user’s responses, where the analytic hierarchy process was employed to quantify the performance of algorithms in terms of multiple criteria. Our results suggest that human in general agree on the evaluation of the performance of algorithms, and some sketch-based mesh cutting methods are consistently more favorable than others. The importance of our work lies in studying users’ experience on operating various sketch-based mesh cutting tools, to motivate more practical interactive systems in the future.",
		"contents": [{
				"name": "",
				"images": [{
					"type":"Figure 1",
					"uri": "images/publications/2012-gm-eval_sketch_seg/teaser.png",
					"caption": " Different user interfaces of various sketch-based mesh cutting algorithms: (a) foreground/background sketch-based interface; (b) foreground sketch-based interface; (c) cross-boundary sketch-based interface; (d) along-boundary sketch-based interface. See detail about these tools in the of Sketch-based Interactive Mesh Cutting. We present a comprehensively comparative study on evaluating the performances of these algorithms and the user experiences of these user interfaces."
				}]
			},
			{
				"name": "Results",
				"images": [{
					"type":"Figure 2",
					"uri": "images/publications/2012-gm-eval_sketch_seg/ui.png",
					"caption": "Screenshot of our evaluation system: (a) the evaluation system for different sketch-based mesh cutting algorithms; (b) the task panel for evaluation."
				}]
			}
		],
		"videos": [
		{
			"imageUri":"",
			"caption": [
			{
				"title":"Youku",
				"uri":"https://v.youku.com/v_show/id_XNDE4NzM1NDM3Mg==.html?spm=a2h3j.8428770.3416059.1"
			},
			{
				"title":"YouTube",
				"uri":""
			}
			]
		}
		],
		"acknowlegement": "This work is supported by the National Natural Science Foundation of China (61070071) and the 973 National Key Basic Research Foundation of China (2009CB320801).",
		"bibtex": {
			"type": "article",
			"name": "Fan:2012:EvalSketchSeg",
			"title": "Sketch-based Mesh Cutting: A Comparative Study",
			"author": "Lubin Fan and Min Meng and Ligang Liu",
			"journel": "Graphical Models",
			"volume": "74",
			"number": "6",
			"pages": "292-301",
			"year": "2012"
		},
		"materials": [{
				"iconUri": "images/icons/pdf_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2012-gm-eval_sketch_seg.pdf",
				"caption": "Paper (1MB)"
			}, {
				"iconUri": "images/icons/pdf_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2012-gm-eval_sketch_seg.suppl.pdf",
				"caption": "Supplementary Materials (1MB)"
			},

			{
				"iconUri": "images/icons/zip_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2012-gm-eval_sketch_seg.data.zip",
				"caption": "Dataset (60.3MB)"
			},
			{
				"iconUri": "images/icons/video_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2012-gm-eval_sketch_seg.mov",
				"caption": "Video (4.2MB)"
			}
		]
	},
	{
		"title": "iCutter: A Direct Cut-out Tool for 3D Shapes",
		"authors": [{
				"fullName": "Min Meng",
				"url": "",
				"afflID": [0]
			},
			{
				"fullName": "Lubin Fan",
				"url": "/",
				"afflID": [0]
			},
			{
				"fullName": "Ligang Liu",
				"url": "http://staff.ustc.edu.cn/~lgliu/",
				"afflID": [0]
			}
		],
		"affiliations": [{
			"name": "Zhejiang University",
			"url": "http://www.zju.edu.cn/english/"
		}],
		"event": "Journal of Computer Animation and Virtual Worlds,  22(4), 335-342, 2011",
		"abstract": "We present a novel sketch-based tool, called iCutter (short for Intelligent Cutter), for cutting out semantic parts of 3D shapes. When a user performs a cutting task, he only needs to draw a freehand stroke to roughly specify where cuts should be made without much attention. Then iCutter intelligently returns the best cut that meets the user’s intention and expectation. We develop a novel scheme for selecting the optimal isoline from a well-designed scalar field induced from the input stroke, which respects the part saliency as well as the input stroke. We demonstrate various examples to illustrate the flexibility and applicability of our iCutter tool.",
		"contents": [{
				"name": "",
				"images": [{
					"type":"Figure 1",
					"uri": "images/publications/2011-casa-icutter/teaser.png",
					"caption": "Two mesh cutting results produced by our iCutter tool. The user only needs to roughly draw a stroke (in purple) around the desired cutting boundary. iCutter obtains the similar cuts no matter how the user draws the strokes in different way."
				}]
			},
			{
				"name": "Results",
				"images": [{
					   "type":"Figure 2",
						"uri": "images/publications/2011-casa-icutter/armodillo.png",
						"caption": "iCutter is robust to users’ inputs, noise on the object (middle), and object poses (right)."
					},
					{
						"type":"Figure 3",
						"uri": "images/publications/2011-casa-icutter/multiple.png",
						"caption": "iCutter can cut out the different parts with complex features on the Neptune model."
					}
				]
			}
		],
		"videos": [
		{
			"imageUri":"images/publications/2011-casa-icutter/video.png",
			"caption": [
			{
				"title":"Youku",
				"uri":"https://v.youku.com/v_show/id_XNDE4NzM1NDU2NA==.html?spm=a2h3j.8428770.3416059.1"
			},
			{
				"title":"YouTube",
				"uri":"https://www.youtube.com/watch?v=L2H-FOxgzno"
			}
			]
		}
		],
		"acknowlegement": "This work is supported by the National Natural Science Foundation of China (61070071), the 973 National Key Basic Research Foundation of China (2009CB320801), and the Fundamental Research Funds for the Central Universities (2010QNA3039).",
		"bibtex": {
			"type": "article",
			"name": "Meng:2011:iCutter",
			"title": "iCutter: A Direct Cut-out Tool for 3D Shapes",
			"author": "Min Meng and Lubin Fan and Ligang Liu",
			"journel": "Journal of Computer Animation and Virtual Worlds",
			"volume": "22",
			"number": "4",
			"pages": "335-342",
			"year": "2011"
		},
		"materials": [{
				"iconUri": "images/icons/pdf_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2011-casa-icutter.pdf",
				"caption": "Paper"
			},
			{
				"iconUri": "images/icons/video_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2011-casa-icutter.wmv",
				"caption": "Video (24.8MB)"
			},
			{
				"iconUri": "images/icons/ppt_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2011-casa-icutter.slides.pdf",
				"caption": "Slides (5.1MB)"
			}
		]
	},
	{
		"title": "Paint Mesh Cutting",
		"authors": [

			{
				"fullName": "Lubin Fan",
				"url": "/",
				"afflID": [0]
			},
			{
				"fullName": "Ligang Liu",
				"url": "http://staff.ustc.edu.cn/~lgliu/",
				"afflID": [0]
			},
			{
				"fullName": "Kun Liu",
				"url": "http://kwunlyou.com/",
				"afflID": [0]
			}
		],
		"affiliations": [{
			"name": "Zhejiang University",
			"url": "http://www.zju.edu.cn/english/"
		}],
		"event": "Computer Graphics Forum (Proc. of Eurographics), 2011",
		"abstract": "We present a novel progressive painting-based mesh cut out tool, called Paint Mesh Cutting, for interactive mesh segmentation. Different from the previous user interfaces, the user only needs to draw a single stroke on the foreground region and then obtains the desired cutting part at an interactive rate. Moreover, the user progressively paints the region of interest using a brush and has the instant feedback on cutting results as he/she drags the mouse. This is achieved by efficient local graph-cut based optimizations based on the Gaussian mixture models (GMM) on the shape diameter function (SDF) metric of the shape. We demonstrate a number of various examples to illustrate the flexibility and applicability of our system and present a user study that supports the advantages of our user interface.",
		"contents": [{
				"name": "",
				"images": [{
					"type":"Figure 1",
					"uri": "images/publications/2011-eg-paint_mesh_cutting/teaser.png",
					"caption": "Illustrations of Paint Mesh Cutting. The user paints the region of interest with a single brush (in blue) on a mesh surface and then obtains the cutting part (in orange). Our system can provide the user instant feedback of the cutting results during mouse dragging (from left to right).Note that we only show 3 snapshots of the continuously sampled point interaction here."
				}]
			},
			{
				"name": "Results",
				"images": [{
					    "type":"Figure 2",
						"uri": "images/publications/2011-eg-paint_mesh_cutting/parts.png",
						"caption": "A complete segmentation example of using our paint brush tool."
					},
					{
						"type":"Figure 3",
						"uri": "images/publications/2011-eg-paint_mesh_cutting/patch.png",
						"caption": "An example of progressive patch-based cutting. Two foreground brushes are used."
					}
				]
			}
		],
		"videos": [
		{
			"imageUri":"images/publications/2011-eg-paint_mesh_cutting/video.png",
			"caption": [
			{
				"title":"Youku",
				"uri":"https://v.youku.com/v_show/id_XNDE4NzM1NDY1Mg==.html?spm=a2h3j.8428770.3416059.1"
			},
			{
				"title":"YouTube",
				"uri":"https://www.youtube.com/watch?v=GrLvbjXMGMw"
			}
			]
		}
		],
		"acknowlegement": "We thank all the reviewers for valuable feedback. We are thankful to Jie Xu for video narration. This work is supported by the National Natural Science Foundation of China (61070071) and the 973 National Key Basic Research Foundation of China (No. 2009CB320801).",
		"bibtex": {
			"type": "article",
			"name": "Fan:2011:PaintMeshCutting",
			"title": "Paint Mesh Cutting",
			"author": "Lubin Fan and Ligang Liu and Kun Liu",
			"journel": "Computer Graphic Forum (Proc. of Eurographics 2011)",
			"volume": "30",
			"number": "2",
			"pages": "603-611",
			"year": "2011"
		},
		"materials": [{
				"iconUri": "images/icons/pdf_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2011-eg-paint_mesh_cutting.pdf",
				"caption": "Paper (4.0MB)"
			},
			{
				"iconUri": "images/icons/video_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2011-eg-paint_mesh_cutting.mov",
				"caption": "Video (23.7MB)"
			},
			{
				"iconUri": "images/icons/ppt_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2011-eg-paint_mesh_cutting.slides.pdf",
				"caption": "Slides (34.0MB)"
			}
		]
	},
	{
		"title": "A Comparative Evaluation of Forground/Background Skech-based Mesh Segmentation Algorithm",
		"authors": [

			{
				"fullName": "Min Meng",
				"url": "",
				"afflID": [0]
			},
			{
				"fullName": "Lubin Fan",
				"url": "/",
				"afflID": [0]
			},
			{
				"fullName": "Ligang Liu",
				"url": "http://staff.ustc.edu.cn/~lgliu/",
				"afflID": [0]
			}
		],
		"affiliations": [{
			"name": "Zhejiang University",
			"url": "http://www.zju.edu.cn/english/"
		}],
		"event": "Shape Modeling International 2011",
		"abstract": "This paper presents an extensive comparative evaluation of five popular foreground/background sketch-based interactive mesh segmentation algorithms, addressing the quantitative assessment of the accuracy, efficiency, and stability of each algorithm. To facilitate the comparison, we have developed a complete framework with an intuitive and simple sketch-based interface to enable interactive mesh segmentation by marking strokes to specify the foreground and background with the mouse buttons, allowing us to quantify the algorithms in a unified manner. The evaluation has been performed via extensive user experiments in which each participant was assigned to segment models with the evaluated algorithms and the corresponding update of each segmentation was recorded as a new refinement when additional interactions were added. We then collected the segmentations from participants and evaluated them against the ground truth corpus constructed from the Princeton segmentation database. To investigate how well the interactive segmentations match the ground-truth, five metrics were used to measure the boundary and region accuracy of segmentations. By studying the experimental results, we have analyzed the performance of the evaluated algorithms and provided valuable insights into their characteristics.",
		"contents": [{
				"name": "",
				"images": [{
					"type":"Figure 1",
					"uri": "images/publications/2011-smi-eval_fb_seg/teaser.png",
					"caption": "Ilustration of foreground/background sketch-based interactive mesh segmentation. Given a bunny model (a), the user marks a blue stroke on the area of the bunny’s head (foreground) and a red stroke on the area of the bunny’s body (background) and obtains a segmentation result in (b). The user then specifies an additional red stroke on the area of the bunny’s body and the algorithm updates the segmentation using the new information, as shown in (c)."
				}]
			},
			{
				"name": "Results",
				"images": [{
					    "type":"Figure 2",
						"uri": "images/publications/2011-smi-eval_fb_seg/ui.png",
						"caption": "A screenshot of our interactive segmentation system.(a)The interactive segmentation system in evaluation mode.(b)Task panel for evaluation.(c)Timer for restricting users to a maximum of 5 min per task. (For interpretation of the references to color in this figure legend, the reader is referred to the article.)"
					},
					{
						"type":"Figure 3",
						"uri": "images/publications/2011-smi-eval_fb_seg/results.png",
						"caption": "Comparison of interactive segmentation algorithms by over all average boundary accuracy and region accuracy.(a)Average boundary accuracy; (b) average region accuracy."
					}
				]
			}
		],
		"videos": [
		{
			"imageUri":"",
			"caption": [
			{
				"title":"Youku",
				"uri":""
			},
			{
				"title":"YouTube",
				"uri":""
			}
			]
		}
		],
		"acknowlegement": "This work is supported by the National Natural Science Foundation of China (61070071), the 973 National Key Basic Research Foundation of China (2009CB320801), and the Fundamental Research Funds for the Central Universities (2010QNA3039).",
		"bibtex": {
			"type": "article",
			"name": "Meng:2011:EvalFBSeg",
			"title": "A Comparative Evaluation of Foreground/Background Sketch-based Mesh Segmentation Algorithms",
			"author": "Min Meng and Lubin Fan and Ligang Liu",
			"journel": "Computers & Graphics (Proc. of Shape Modeling International 2011)",
			"volume": "35",
			"number": "3",
			"pages": "650-660",
			"year": "2011"
		},
		"materials": [{
				"iconUri": "images/icons/pdf_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2011-smi-eval_fb_seg.pdf",
				"caption": "Paper (1.3MB)"
			},
			{
				"iconUri": "images/icons/ppt_wh64.png",
				"matUri": "https://lbfan.oss-cn-hangzhou.aliyuncs.com/personal_webpage/publications/2011-smi-eval_fb_seg.slides.pdf",
				"caption": "Slides (6.6MB)"
			}
		]
	}
]