[
	{
		"title": "MIQP-based Layout Design for Building Interiors",
		"authors": 
		[
			{
				"fullName": "Wenming Wu",
				"url": "https://wutomwu.github.io/",
				"afflID": [0, 1]
			},
			{
				"fullName": "Lubin Fan",
				"url": "https://lubinfan.github.io/",
				"afflID": [0]
			},
			{
				"fullName": "Ligang Liu",
				"url": "http://staff.ustc.edu.cn/~lgliu/",
				"afflID": [1]
			},
			{
				"fullName": "Peter Wonka",
				"url": "http://peterwonka.net/",
				"afflID": [0]
			}
		],
		"affiliations": 
		[
			{
				"name": "King Abdullah University of Science and Technology"
			},
			{
				"name": "University of Science and Technology of China"
			}
		],
		"event": "Computer Grahpics Forum (Proc. of Eurographics 2018)",
		"abstract": "We propose a hierarchical framework for the generation of building interiors. Our solution is based on a mixed integer quadratic programming (MIQP) formulation. We parametrize a layout by polygons that are further decomposed into small rectangles. We identify important high-level constraints, such as room size, room position, room adjacency, and the outline of the building, and formulate them in a way that is compatible with MIQP and the problem parametrization. We also propose a hierarchical framework to improve the scalability of the approach. We demonstrate that our algorithm can be used for residential building layouts and can be scaled up to large layouts such as office buildings, shopping malls, and supermarkets. We show that our method is faster by multiple orders of magnitude than previous methods.",
		"contents": 
		[
			{
				"name": "",
				"images": [{
					"type":"Figure 1",
					"uri": "images/2018-IPLayout/teaser.png",
					"caption": "We propose a framework that generates building interiors with high-level constraints, e.g., room size, room position, room adjacency, and the outline of the building. (a) The layout of a two-storey house and corresponding 3D renderings. (b) A large-scale example depicting an office building. For such large-scale layouts, our method is faster by multiple orders of magnitude than previous methods."
				}]
			},
			{
				"name": "Overview",
				"images": 
				[
					{
						"type":"Figure 2",
						"uri": "images/2018-IPLayout/overview.png",
						"caption": "Overview of our framework. Given a list of high-level constraints and an outline as input (Input), we compute a layout using a hierarchical framework. Level 1 shows an initial layout and Level 2 shows the layout including local refinements. Finally, additional details can be added to visualize the results as architectural floorplans or 3D models."
						}
					]
			},
			{
				"name": "Results",
				"images": [{
					    "type":"Figure 3",
						"uri": "images/2018-IPLayout/result1.png",
						"caption": "Two apartment layouts are generated by our algorithm with the same input. Boundary constraints are added for bedrooms to ensure that all bedrooms receive sufficient sunshine from the south east direction. The north direction is shown."
					},
					{ 
						"type":"Figure 4",
						"uri": "images/2018-IPLayout/result2.png",
						"caption": "An interior layout of a bungalow-style house generated by our algorithm. The floorplan is shown in the middle. Two sides of the 3D rendering are shown."
					},
					{
						"type":"Figure 5",
						"uri": "images/2018-IPLayout/result3.png",
						"caption": "An interior layout of a two-storey house. The stairs are considered as special rooms that should be consistent between two floors. The floorplan of each floor is shown on the left, 3D renderings are shown on the right."
					},
					{ 
						"type":"Figure 6",
						"uri": "images/2018-IPLayout/result4.png",
						"caption": "Interior layouts of office buildings generated by our method. Polygons in the same color are from the same parent rectangle in a higher level."
					},
					{
						"type":"Figure 7",
						"uri": "images/2018-IPLayout/result5.png",
						"caption": "Interior layouts of supermarkets generated by our method. Shelves for products in the same category are in the same color."
					}
				]
			}
		],
		"acknowlegement": "This work was supported by the KAUST Office of Sponsored Research (OSR) under Award No. OCRF-2014-CGR3-62140401, and the Visual Computing Center at KAUST. Ligang Liu is supported by the National Natural Science Foundation of China (61672482, 61672481, 11626253) and the One Hundred Talent Project of the Chinese Academy of Sciences. We would like to thank Virginia Unkefer for proofreading the paper.",
		"bibtex": 
		{
			"type": "article",
			"name": "wu2018miqp",
			"title": "MIQP-based Layout Design for Building Interiors",
			"author": "Wenming Wu and Lubin Fan and Ligang Liu and Peter Wonka",
			"journel": "Computer Grahpics Forum (SIGGRAPH Asia)",
			"volume": "37",
			"number": "2",
			"pages": "511--521",
			"year": "2018"
		},
		"materials": 
		[ 
			{
				"iconUri": "images/icons/pdf_wh64.png",
				"matUri": "images/2018-IPLayout/paper.pdf",
				"caption": "Paper (~8MB)"
			},
			{
				"iconUri": "images/icons/pdf_wh64.png",
				"matUri": "images/2018-IPLayout/suppl.pdf",
				"caption": "Supplementary Material (~3MB)"
			},
			{
				"iconUri": "images/icons/video_wh64.png",
				"matUri": "images/2018-IPLayout/video.pdf",
				"caption": "Video (~50MB)"
			}
		]
	},
	{
		"title": "Data-driven Interior Plan Generation for Residential Buildings",
		"authors": 
		[
			{
				"fullName": "Wenming Wu",
				"url": "https://wutomwu.github.io/",
				"afflID": [0]
			},
			{
				"fullName": "Xiao-Ming Fu",
				"url": "https://ustc-gcl-f.github.io/",
				"afflID": [0]
			},
			{
				"fullName": "Rui Tang",
				"afflID": [1]
			},
			{
				"fullName": "Yuhan Wang",
				"afflID": [1]
			},
			{
				"fullName": "Yu-Hao Qi",
				"afflID": [1]
			},
			{
				"fullName": "Ligang Liu",
				"url": "http://staff.ustc.edu.cn/~lgliu/",
				"afflID": [0]
			}
		],
		"affiliations": 
		[
			{
				"name": "University of Science and Technology of China"
			},
			{
				"name": "Kujiale"
			}
		],
		"event": "ACM Transactions on Graphics (Proc. of SIGGRAPH Asia 2019)",
		"abstract": "We propose a novel data-driven technique for automatically and efficiently generating floor plans for residential buildings with given boundaries. Central to this method is a two-stage approach that imitates the human design process by locating rooms first and then walls while adapting to the input building boundary. Based on observations of the presence of the living room in almost all fl oor plans, our designed learning network begins with positioning a living room and continues by iteratively generating other rooms. Then, walls are first determined by an encoder-decoder network, and then they are refined to vector representations using dedicated rules. To effectively train our networks, we construct RPLAN - a manually collected large-scale densely annotated dataset of floor plans from real residential buildings. Intensive experiments, including formative user studies and comparisons, are conducted to illustrate the feasibility and efficacy of our proposed approach. By comparing the plausibility of different floor plans, we have observed that our method substantially outperforms existing methods, and in many cases our floor plans are comparable to human-created ones.",
		"contents": 
		[
			{
				"name": "",
				"images": 
				[
					{
						"type":"Figure 1",
						"uri": "images/2018-IPLayout/teaser.png",
						"caption": "Floor plan for one residential building generated by our approach, given a boundary as input."
					}
				]
			},
			{
				"name": "Overview",
				"images": 
				[
					{
						"type":"Figure 2",
						"uri": "images/2019-DeepLayout/overview.png",
						"caption": "Overview of our method. Given an input boundary (a), our method first uses an iterative prediction model to obtain room locations (red dots in b). Based on the this, our method further locates walls (shown in blue) to obtain a vectorized floor plan (c). In this step, our method uses an encoder-decoder network to predict wall locations and then a post-processing step converts the floor plan into the final vector format. Additional details, including doors and windows, are added to visualize the floor plan (d)."
					}
				]
			},
			{
				"name": "Results",
				"images": 
				[
					{
					    "type":"Figure 3",
						"uri": "images/2019-DeepLayout/dataset.png",
						"caption": "Our dataset RPLAN. (a) Statistics on the occurrence of each room type (a1), room number per floor plan (a2), the proportion of the area of the living room (a3), and the number of floor plan according to the total area (a4). (b) One typical floor plan in our collected dataset. (c) For each floor plan, we abstract the necessary information used in our method, including the boundary mask (the entrance is shown in red), inside mask, (interior) wall mask, and room mask."
					},
					{ 
						"type":"Figure 4",
						"uri": "images/2019-DeepLayout/result1.png",
						"caption": "Examples of floor plans synthesized by our method, given one or two room locations specified by users (red dots). From lef to right: no specified room, specified master room, and specified kitchen and bathroom."
					},
					{
						"type":"Figure 5",
						"uri": "images/2019-DeepLayout/result2.png",
						"caption": "Comparison to the nearest neighbors in the training dataset. Top row: the nearest neighbors. Botom row: our synthesized results."
					},
					{ 
						"type":"Figure 6",
						"uri": "images/2019-DeepLayout/result3.png",
						"caption": "Floor plan generation for non axis-aligned boundaries. In the botom row, the examples contain curved walls."
					},
					{
						"type":"Figure 7",
						"uri": "images/2018-IPLayout/result5.png",
						"caption": "Synthesizing multiple floor plans given the same boundary as input."
					}
				]
			}
		],
		"acknowlegement": "We would like to thank Kai Wang for providing their implementation of [Wang et al. 2018], user study participants for evaluating our results, and the anonymous reviewers for their constructive suggestions and comments. This work is supported by the National Natural Science Foundation of China (61802359, 61672482, 11626253) and the Fundamental Research Funds for the Central Universities (WK0010460006, WK0010450004).",
		"bibtex": 
		{
			"type": "article",
			"name": "wu2019data",
			"title": "MIQP-based Layout Design for Building Interiors",
			"author": "Wenming Wu and Lubin Fan and Ligang Liu and Peter Wonka",
			"journel": "ACM Transactions on Graphics (SIGGRAPH Asia)",
			"volume": "38",
			"number": "6",
			"year": "2019"
		},
		"materials": 
		[ 
			{
				"iconUri": "images/icons/pdf_wh64.png",
				"matUri": "images/2019-DeepLayout/paper.pdf",
				"caption": "Paper (~24MB)"
			},
			{
				"iconUri": "images/icons/pdf_wh64.png",
				"matUri": "images/2019-DeepLayout/suppl.pdf",
				"caption": "Supplementary Material (~7MB)"
			},
			{
				"iconUri": "images/icons/video_wh64.png",
				"matUri": "images/2019-DeepLayout/video.pdf",
				"caption": "Video (~12MB)"
			}
		]
	},
	{
		"title": "Tailored Reality: Perception-Aware Scene Restructuring for Adaptive VR Navigation",
		"authors": 
		[
			{
				"fullName": "Zhi-Chao Dong",
				"afflID": [0]
			},
			{
				"fullName": "Wenming Wu",
				"url": "https://wutomwu.github.io/",
				"afflID": [0]
			},
			{
				"fullName": "Zeng-Hao Xu",
				"afflID": [0]
			},
			{
				"fullName": "Qi Sun",
				"afflID": [1]
			},
			{
				"fullName": "Guan-Jie Yuan",
				"afflID": [0]
			},
			{
				"fullName": "Ligang Liu",
				"url": "http://staff.ustc.edu.cn/~lgliu/",
				"afflID": [0]
			},
			{
				"fullName": "Ligang Liu",
				"url": "https://ustc-gcl-f.github.io/",
				"afflID": [0]
			}
		],
		"affiliations": 
		[
			{
				"name": "University of Science and Technology of China"
			},
			{
				"name": "New York University"
			}
		],
		"event": "ACM Transactions on Graphics (Proc. of SIGGRAPH Asia 2019)",
		"abstract": "In virtual reality (VR), the virtual scenes are pre-designed by creators. Our physical surroundings, however, comprise signifcantly varied sizes, layouts, and components. To bridge the gap and further enable natural navigation, recent solutions have been proposed to redirect users or recreate the virtual content. However, they suﬀer from either interrupted experience or distorted appearances. We present a novel VR-oriented algorithm that automatically restructures a given virtual scene for a user’s physical environment. Diﬀerent from the previous methods, we introduce neither interrupted walking experience nor curved appearances. Instead, a perceptionaware function optimizes our retargeting technique to preserve the fdelity of the virtual scene that appears in VR head-mounted displays. Besides geometric and topological properties, it emphasizes the unique frst-person view perceptual factors in VR, such as dynamic visibility and objectwise relationships. We conduct both analytical experiments and subjective studies. The results demonstrate our system’s versatile capability and practicability for natural navigation in VR: It reduces the virtual space by 40% without statistical loss of perceptual identicality.", 
		"contents": 
		[
			{
				"name": "",
				"images": 
				[
					{
						"type":"Figure 1",
						"uri": "images/2021-TailoredReality/teaser.png",
						"caption": "Illustration of our system. Tailored for first-person view VR applications, we propose a novel perception-aware scene retargeting method. It restructures a given virtual scene (a) for users’ small and varied physical spaces (valid area in (b) and (c)). Our technique preserves not only geometric and topological properties but also perceptual fidelity with the immersive first-person view. It addresses the commonly existing collision problems from previous method. For instance the red areas in (e). This can be visualized by comparing the sampled HMD views between Huang et al. [2016] (e) and ours (f)."
					}
				]
			},
			{
				"name": "Results",
				"images": 
				[
					{
					    "type":"Figure 2",
						"uri": "images/2021-TailoredReality/results1.png",
						"caption": "Diﬀerent weight sets of the energy function Er ({si }). The virtual scene in Figure 1(a) is retargeted into the same real space, and the ratio of real space is 0.65. The weights for (Escale, Eobject, Escene, Espace, Evisibility) are set to (0.001, 200, 50, 0.0003, 0.001) (a), (0.0002, 100, 25, 0.0002, 0.002) (b), and (0.0015, 150, 75, 0.0005, 0.003) (c), respectively. The only subtle variances with diﬀerent parameter seting reveals the robustness of the retargeting method."
					},
					{ 
						"type":"Figure 4",
						"uri": "images/2021-TailoredReality/result1.png",
						"caption": "Examples of floor plans synthesized by our method, given one or two room locations specified by users (red dots). From lef to right: no specified room, specified master room, and specified kitchen and bathroom."
					},
					{
						"type":"Figure 5",
						"uri": "images/2021-TailoredReality/result2.png",
						"caption": "Various numbers of salient objects. From lef to right, the number of salient objects in the virtual scene decreases monotonously."
					},
					{ 
						"type":"Figure 6",
						"uri": "images/2021-TailoredReality/result3.png",
						"caption": "Diﬀerent real spaces with the same area. The values above the retargeted scene indicate the ratios between the width and height of the real space and the input virtual scene."
					},
					{
						"type":"Figure 7",
						"uri": "images/2021-TailoredReality/result4.png",
						"caption": "Retargeting results of diﬀerent sizes from 0.75 to 0.45. The first one is shown as the origin scene. Corresponding HMD views at the same position are also shown for each scene. As the retargeting size decreases, the visual distortion of the scene becomes more and more pronounced. The object-level distortion along with the compression stress level is ploted in Figure 20. The objective metrics of the retargeting results are shown in then supplementary materials."
					}
				]
			}
		],
		"acknowlegement": "We thank user study participants for evaluating our system and the anonymous reviewers for their constructive suggestions and comments. This work is supported by the National Natural Science Foundation of China (61802359 and 62025207), Zhejiang Lab (NO.2019NB0AB03), and USTC Research Funds of Double First-Class Initiative (YD0010002003).",
		"bibtex": 
		{
			"type": "article",
			"name": "wu2019data",
			"title": "MIQP-based Layout Design for Building Interiors",
			"author": "Zhi-Chao Dong and Wenming Wu and Zeng-Hao Xu and Qi Sun and Guan-Jie Yuan and Ligang Liu and Xiao-Ming Fu",
			"journel": "ACM Transactions on Graphics",
			"volume": "40",
			"number": "5",
			"year": "2021"
		},
		"materials": 
		[ 
			{
				"iconUri": "images/icons/pdf_wh64.png",
				"matUri": "images/2021-TailoredReality/paper.pdf",
				"caption": "Paper (~22MB)"
			},
			{
				"iconUri": "images/icons/video_wh64.png",
				"matUri": "images/2021-TailoredReality/video.pdf",
				"caption": "Video (~52MB)"
			},
			{
				"iconUri": "images/icons/zip_wh64.png",
				"matUri": "images/2021-TailoredReality/code.pdf",
				"caption": "Code (~20MB)"
			}
		]
	}
]