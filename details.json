[
	{
		"id": 0,
		"title": "MIQP-based Layout Design for Building Interiors",
		"authors": 
		[
			{
				"fullName": "Wenming Wu",
				"url": "/",
				"afflID": [0, 1]
			},
			{
				"fullName": "Lubin Fan",
				"url": "https://lubinfan.github.io",
				"afflID": [1]
			},
			{
				"fullName": "Ligang Liu",
				"url": "http://staff.ustc.edu.cn/~lgliu",
				"afflID": [0]
			},
			{
				"fullName": "Peter Wonka",
				"url": "http://peterwonka.net",
				"afflID": [1]
			}
		],
		"affiliations": 
		[
			{
				"name": "University of Science and Technology of China",
				"url": "http://www.ustc.edu.cn"
			},
			{
				"name": "King Abdullah University of Science and Technology",
				"url": "http://www.kaust.edu.sa"
			}
		],
		"event": "Computer Grahpics Forum (Proc. of Eurographics 2018)",
		"abstract": "We propose a hierarchical framework for the generation of building interiors. Our solution is based on a mixed integer quadratic programming (MIQP) formulation. We parametrize a layout by polygons that are further decomposed into small rectangles. We identify important high-level constraints, such as room size, room position, room adjacency, and the outline of the building, and formulate them in a way that is compatible with MIQP and the problem parametrization. We also propose a hierarchical framework to improve the scalability of the approach. We demonstrate that our algorithm can be used for residential building layouts and can be scaled up to large layouts such as office buildings, shopping malls, and supermarkets. We show that our method is faster by multiple orders of magnitude than previous methods.",
		"contents": 
		[
			{
				"name": "Teaser",
				"images": 
				[
					{
						"type":"Figure 1",
						"uri": "publications/2018-IPLayout/teaser.jpg",
						"caption": "We propose a framework that generates building interiors with high-level constraints, e.g., room size, room position, room adjacency, and the outline of the building. (a) The layout of a two-storey house and corresponding 3D renderings. (b) A large-scale example depicting an office building. For such large-scale layouts, our method is faster by multiple orders of magnitude than previous methods."
					}
				]
			},
			{
				"name": "Overview",
				"images": 
				[
					{
						"type":"Figure 2",
						"uri": "publications/2018-IPLayout/overview.jpg",
						"caption": "Overview of our framework. Given a list of high-level constraints and an outline as input (Input), we compute a layout using a hierarchical framework. Level 1 shows an initial layout and Level 2 shows the layout including local refinements. Finally, additional details can be added to visualize the results as architectural floorplans or 3D models."
					}
				]
			},
			{
				"name": "Results",
				"images": 
				[
					{
					    "type":"Figure 3",
						"uri": "publications/2018-IPLayout/result1.jpg",
						"caption": "Two apartment layouts are generated by our algorithm with the same input. Boundary constraints are added for bedrooms to ensure that all bedrooms receive sufficient sunshine from the south east direction. The north direction is shown."
					},
					{ 
						"type":"Figure 4",
						"uri": "publications/2018-IPLayout/result2.jpg",
						"caption": "An interior layout of a bungalow-style house generated by our algorithm. The floorplan is shown in the middle. Two sides of the 3D rendering are shown."
					},
					{
						"type":"Figure 5",
						"uri": "publications/2018-IPLayout/result3.jpg",
						"caption": "An interior layout of a two-storey house. The stairs are considered as special rooms that should be consistent between two floors. The floorplan of each floor is shown on the left, 3D renderings are shown on the right."
					},
					{ 
						"type":"Figure 6",
						"uri": "publications/2018-IPLayout/result4.jpg",
						"caption": "Interior layouts of office buildings generated by our method. Polygons in the same color are from the same parent rectangle in a higher level."
					},
					{
						"type":"Figure 7",
						"uri": "publications/2018-IPLayout/result5.jpg",
						"caption": "Interior layouts of supermarkets generated by our method. Shelves for products in the same category are in the same color."
					}
				]
			}
		],
		"acknowlegement": "This work was supported by the KAUST Office of Sponsored Research (OSR) under Award No. OCRF-2014-CGR3-62140401, and the Visual Computing Center at KAUST. Ligang Liu is supported by the National Natural Science Foundation of China (61672482, 61672481, 11626253) and the One Hundred Talent Project of the Chinese Academy of Sciences. We would like to thank Virginia Unkefer for proofreading the paper.",
		"bibtex": 
		{
			"type": "article",
			"name": "wu2018miqp",
			"title": "MIQP-based Layout Design for Building Interiors",
			"author": "Wenming Wu and Lubin Fan and Ligang Liu and Peter Wonka",
			"journel": "Computer Grahpics Forum (SIGGRAPH Asia)",
			"volume": "37",
			"number": "2",
			"pages": "511--521",
			"year": "2018"
		},
		"materials": 
		[ 
			{
				"iconUri": "publications/icons/pdf_wh64.png",
				"matUri": "https://drive.google.com/file/d/1dgiRYRmBRnHzRu938kY7Krqu77dY-1LX/view?usp=share_link",
				"caption": "Paper (~108MB)"
			},
			{
				"iconUri": "publications/icons/pdf_wh64.png",
				"matUri": "publications/2018-IPLayout/paper_low_res.pdf",
				"caption": "Low-res Paper (~8MB)"
			},
			{
				"iconUri": "publications/icons/pdf_wh64.png",
				"matUri": "publications/2018-IPLayout/suppl.pdf",
				"caption": "Supplementary Material (~3MB)"
			},
			{
				"iconUri": "publications/icons/video_wh64.png",
				"matUri": "publications/2018-IPLayout/video.mp4",
				"caption": "Video (~50MB)"
			},
			{
				"iconUri": "publications/icons/ppt_wh64.png",
				"matUri": "publications/2018-IPLayout/slide.pptx",
				"caption": "Slide (~18MB)"
			}
		]
	},
	{
		"id": 1,
		"title": "Data-driven Interior Plan Generation for Residential Buildings",
		"authors": 
		[
			{
				"fullName": "Wenming Wu",
				"url": "/",
				"afflID": [0]
			},
			{
				"fullName": "Xiao-Ming Fu",
				"url": "https://ustc-gcl-f.github.io",
				"afflID": [0]
			},
			{
				"fullName": "Rui Tang",
				"afflID": [1]
			},
			{
				"fullName": "Yuhan Wang",
				"afflID": [1]
			},
			{
				"fullName": "Yu-Hao Qi",
				"afflID": [0]
			},
			{
				"fullName": "Ligang Liu",
				"url": "http://staff.ustc.edu.cn/~lgliu",
				"afflID": [0]
			}
		],
		"affiliations": 
		[
			{
				"name": "University of Science and Technology of China",
				"url": "http://www.ustc.edu.cn"
			},
			{
				"name": "Kujiale",
				"url": "https://www.kujiale.com"
			}
		],
		"event": "ACM Transactions on Graphics (Proc. of SIGGRAPH Asia 2019)",
		"abstract": "We propose a novel data-driven technique for automatically and efficiently generating floor plans for residential buildings with given boundaries. Central to this method is a two-stage approach that imitates the human design process by locating rooms first and then walls while adapting to the input building boundary. Based on observations of the presence of the living room in almost all fl oor plans, our designed learning network begins with positioning a living room and continues by iteratively generating other rooms. Then, walls are first determined by an encoder-decoder network, and then they are refined to vector representations using dedicated rules. To effectively train our networks, we construct RPLAN - a manually collected large-scale densely annotated dataset of floor plans from real residential buildings. Intensive experiments, including formative user studies and comparisons, are conducted to illustrate the feasibility and efficacy of our proposed approach. By comparing the plausibility of different floor plans, we have observed that our method substantially outperforms existing methods, and in many cases our floor plans are comparable to human-created ones.",
		"contents": 
		[
			{
				"name": "Teaser",
				"images": 
				[
					{
						"type":"Figure 1",
						"uri": "publications/2019-DeepLayout/teaser.jpg",
						"caption": "Our dataset RPLAN. (a) Statistics on the occurrence of each room type (a1), room number per floor plan (a2), the proportion of the area of the living room (a3), and the number of floor plan according to the total area (a4). (b) One typical floor plan in our collected dataset. (c) For each floor plan, we abstract the necessary information used in our method, including the boundary mask (the entrance is shown in red), inside mask, (interior) wall mask, and room mask."
					}
				]
			},
			{
				"name": "Overview",
				"images": 
				[
					{
						"type":"Figure 2",
						"uri": "publications/2019-DeepLayout/overview.jpg",
						"caption": "Overview of our method. Given an input boundary (a), our method first uses an iterative prediction model to obtain room locations (red dots in b). Based on the this, our method further locates walls (shown in blue) to obtain a vectorized floor plan (c). In this step, our method uses an encoder-decoder network to predict wall locations and then a post-processing step converts the floor plan into the final vector format. Additional details, including doors and windows, are added to visualize the floor plan (d)."
					}
				]
			},
			{
				"name": "Results",
				"images": 
				[
					{ 
						"type":"Figure 3",
						"uri": "publications/2019-DeepLayout/result1.jpg",
						"caption": "Examples of floor plans synthesized by our method, given one or two room locations specified by users (red dots). From lef to right: no specified room, specified master room, and specified kitchen and bathroom."
					},
					{
						"type":"Figure 4",
						"uri": "publications/2019-DeepLayout/result2.jpg",
						"caption": "Comparison to the nearest neighbors in the training dataset. Top row: the nearest neighbors. Botom row: our synthesized results."
					},
					{ 
						"type":"Figure 5",
						"uri": "publications/2019-DeepLayout/result3.jpg",
						"caption": "Floor plan generation for non axis-aligned boundaries. In the botom row, the examples contain curved walls."
					},
					{
						"type":"Figure 6",
						"uri": "publications/2019-DeepLayout/result4.jpg",
						"caption": "Synthesizing multiple floor plans given the same boundary as input."
					}
				]
			}
		],
		"acknowlegement": "We would like to thank Kai Wang for providing their implementation of [Wang et al. 2018], user study participants for evaluating our results, and the anonymous reviewers for their constructive suggestions and comments. This work is supported by the National Natural Science Foundation of China (61802359, 61672482, 11626253) and the Fundamental Research Funds for the Central Universities (WK0010460006, WK0010450004).",
		"bibtex": 
		{
			"type": "article",
			"name": "wu2019data",
			"title": "MIQP-based Layout Design for Building Interiors",
			"author": "Wenming Wu and Lubin Fan and Ligang Liu and Peter Wonka",
			"journel": "ACM Transactions on Graphics (SIGGRAPH Asia)",
			"volume": "38",
			"number": "6",
			"pages": "1--12",
			"year": "2019"
		},
		"materials": 
		[ 
			{
				"iconUri": "publications/icons/pdf_wh64.png",
				"matUri": "publications/2019-DeepLayout/paper.pdf",
				"caption": "Paper (~24MB)"
			},
			{
				"iconUri": "publications/icons/pdf_wh64.png",
				"matUri": "publications/2019-DeepLayout/suppl.pdf",
				"caption": "Supplementary Material (~7MB)"
			},
			{
				"iconUri": "publications/icons/video_wh64.png",
				"matUri": "publications/2019-DeepLayout/video.mp4",
				"caption": "Video (~12MB)"
			},
			{
				"iconUri": "publications/icons/ppt_wh64.png",
				"matUri": "publications/2019-DeepLayout/slide.pptx",
				"caption": "Slide (~10MB)"
			},
			{
				"iconUri": "publications/icons/dataset_wh64.png",
				"matUri": "https://docs.google.com/forms/d/e/1FAIpQLSfwteilXzURRKDI5QopWCyOGkeb_CFFbRwtQ0SOPhEg0KGSfw/viewform?usp=sf_link",
				"caption": "Dataset"
			},
			{
				"iconUri": "publications/icons/zip_wh64.png",
				"matUri": "https://drive.google.com/file/d/1bMURobr16oeD7wT8jbVuUIJPgOTtOYWs/view?usp=share_link",
				"caption": "Code (~155MB)"
			}
		]
	},
	{
		"id": 2,
		"title": "Tailored Reality: Perception-Aware Scene Restructuring for Adaptive VR Navigation",
		"authors": 
		[
			{
				"fullName": "Zhi-Chao Dong",
				"afflID": [0]
			},
			{
				"fullName": "Wenming Wu",
				"url": "/",
				"afflID": [0]
			},
			{
				"fullName": "Zeng-Hao Xu",
				"afflID": [0]
			},
			{
				"fullName": "Qi Sun",
				"url": "https://qisun.me",
				"afflID": [1]
			},
			{
				"fullName": "Guan-Jie Yuan",
				"afflID": [0]
			},
			{
				"fullName": "Ligang Liu",
				"url": "http://staff.ustc.edu.cn/~lgliu",
				"afflID": [0]
			},
			{
				"fullName": "Xiao-Ming Fu",
				"url": "https://ustc-gcl-f.github.io",
				"afflID": [0]
			}
		],
		"affiliations": 
		[
			{
				"name": "University of Science and Technology of China",
				"url": "http://www.ustc.edu.cn"
			},
			{
				"name": "New York University",
				"url": "https://www.nyu.edu"
			}
		],
		"event": "ACM Transactions on Graphics (Proc. of SIGGRAPH Asia 2019)",
		"abstract": "In virtual reality (VR), the virtual scenes are pre-designed by creators. Our physical surroundings, however, comprise signifcantly varied sizes, layouts, and components. To bridge the gap and further enable natural navigation, recent solutions have been proposed to redirect users or recreate the virtual content. However, they suffer from either interrupted experience or distorted appearances. We present a novel VR-oriented algorithm that automatically restructures a given virtual scene for a user’s physical environment. Different from the previous methods, we introduce neither interrupted walking experience nor curved appearances. Instead, a perceptionaware function optimizes our retargeting technique to preserve the fdelity of the virtual scene that appears in VR head-mounted displays. Besides geometric and topological properties, it emphasizes the unique first-person view perceptual factors in VR, such as dynamic visibility and objectwise relationships. We conduct both analytical experiments and subjective studies. The results demonstrate our system’s versatile capability and practicability for natural navigation in VR: It reduces the virtual space by 40% without statistical loss of perceptual identicality.", 
		"contents": 
		[
			{
				"name": "Teaser",
				"images": 
				[
					{
						"type":"Figure 1",
						"uri": "publications/2021-TailoredReality/teaser.jpg",
						"caption": "Illustration of our system. Tailored for first-person view VR applications, we propose a novel perception-aware scene retargeting method. It restructures a given virtual scene (a) for users’ small and varied physical spaces (valid area in (b) and (c)). Our technique preserves not only geometric and topological properties but also perceptual fidelity with the immersive first-person view. It addresses the commonly existing collision problems from previous method. For instance the red areas in (e). This can be visualized by comparing the sampled HMD views between Huang et al. [2016] (e) and ours (f)."
					}
				]
			},
			{
				"name": "Results",
				"images": 
				[
					{ 
						"type":"Figure 2",
						"uri": "publications/2021-TailoredReality/result1.jpg",
						"caption": "The original indoor scene and the retargeted result using our method. Top: Floor plans of the original indoor scene and our result. Middle: several sampling HMD views from the original indoor scene. Botom: Corresponding views from our result."
					},
					{
					    "type":"Figure 3",
						"uri": "publications/2021-TailoredReality/result2.jpg",
						"caption": "Various numbers of salient objects. From lef to right, the number of salient objects in the virtual scene decreases monotonously."
					},
					{ 
						"type":"Figure 4",
						"uri": "publications/2021-TailoredReality/result3.jpg",
						"caption": "Different real spaces with the same area. The values above the retargeted scene indicate the ratios between the width and height of the real space and the input virtual scene."
					},
					{
						"type":"Figure 5",
						"uri": "publications/2021-TailoredReality/result4.jpg",
						"caption": "Retargeting results of different sizes from 0.75 to 0.45. The first one is shown as the origin scene. Corresponding HMD views at the same position are also shown for each scene. As the retargeting size decreases, the visual distortion of the scene becomes more and more pronounced. The object-level distortion along with the compression stress level is ploted in Figure 20. The objective metrics of the retargeting results are shown in then supplementary materials."
					}
				]
			}
		],
		"acknowlegement": "We thank user study participants for evaluating our system and the anonymous reviewers for their constructive suggestions and comments. This work is supported by the National Natural Science Foundation of China (61802359 and 62025207), Zhejiang Lab (NO.2019NB0AB03), and USTC Research Funds of Double First-Class Initiative (YD0010002003).",
		"bibtex": 
		{
			"type": "article",
			"name": "dong2021tailored",
			"title": "Tailored Reality: Perception-aware Scene Restructuring for Adaptive VR Navigation",
			"author": "Zhi-Chao Dong and Wenming Wu and Zeng-Hao Xu and Qi Sun and Guan-Jie Yuan and Ligang Liu and Xiao-Ming Fu",
			"journel": "ACM Transactions on Graphics",
			"volume": "40",
			"number": "5",
			"pages": "1--15",
			"year": "2021"
		},
		"materials": 
		[ 
			{
				"iconUri": "publications/icons/pdf_wh64.png",
				"matUri": "publications/2021-TailoredReality/paper.pdf",
				"caption": "Paper (~22MB)"
			},
			{
				"iconUri": "publications/icons/video_wh64.png",
				"matUri": "publications/2021-TailoredReality/video.mp4",
				"caption": "Video (~52MB)"
			},
			{
				"iconUri": "publications/icons/ppt_wh64.png",
				"matUri": "https://docs.google.com/presentation/d/1KJ4_pZ240w8eBiMDjDVb7t1ceOjinkJd/edit?usp=share_link&ouid=104838625643552629233&rtpof=true&sd=true",
				"caption": "Slide (~343MB)"
			},
			{
				"iconUri": "publications/icons/zip_wh64.png",
				"matUri": "publications/2021-TailoredReality/code.zip",
				"caption": "Code (~20MB)"
			}
		]
	},
	{
		"id": 3,
		"title": "WallPlan: Synthesizing Floorplans by Learning to Generate Wall Graphs",
		"authors": 
		[
			{
				"fullName": "Jiahui Sun",
				"url": "https://cgjiahui.github.io/",
				"afflID": [0]
			},
			{
				"fullName": "Wenming Wu",
				"url": "/",
				"afflID": [0]
			},
			{
				"fullName": "Ligang Liu",
				"url": "http://staff.ustc.edu.cn/~lgliu",
				"afflID": [1]
			},
			{
				"fullName": "Wenjie Min",
				"afflID": [0]
			},
			{
				"fullName": "Gaofeng Zhang",
				"url": "http://faculty.hfut.edu.cn/zhanggaofeng/zh_CN/index.htm",
				"afflID": [0]
			},
			{
				"fullName": "Liping Zheng",
				"url": "http://faculty.hfut.edu.cn/zhenglp/zh_CN/index.htm",
				"afflID": [0]
			}
		],
		"affiliations": 
		[
			{
				"name": "Hefei University of Technology",
				"url": "https://www.hfut.edu.cn"
			},
			{
				"name": "University of Science and Technology of China",
				"url": "http://www.ustc.edu.cn"
			}
		],
		"event": "ACM Transactions on Graphics (Proc. of SIGGRAPH 2022)",
		"abstract": "Floorplan generation has drawn widespread interest in the community. Recent learning-based methods for generating realistic floorplans have made signifcant progress while a complex heuristic post-processing is still necessary to obtain desired results. In this paper, we propose a novel wall-oriented method, called WallPlan, for automatically and efciently generating plausible floorplans from various design constraints. We pioneer the representation of the floorplan as a wall graph with room labels and consider the floorplan generation as a graph generation. Given the boundary as input, we first initialize the boundary with windows predicted by WinNet. Then a graph generation network GraphNet and semantics prediction network LabelNet are coupled to generate the wall graph progressively by imitating graph traversal. WallPlan can be applied for practical architectural designs, especially the wall-based constraints. We conduct ablation experiments, qualitative evaluations, quantitative comparisons, and perceptual studies to evaluate our method’s feasibility, efcacy, and versatility. Intensive experiments demonstrate our method requires no post-processing, producing higher quality floorplans than state-of-the-art techniques.", 
		"contents": 
		[
			{
				"name": "Teaser",
				"images": 
				[
					{
						"type":"Figure 1",
						"uri": "publications/2022-WallPlan/teaser.jpg",
						"caption": "Illustration of our system. Tailored for first-person view VR applications, we propose a novel perception-aware scene retargeting method. It restructures a given virtual scene (a) for users’ small and varied physical spaces (valid area in (b) and (c)). Our technique preserves not only geometric and topological properties but also perceptual fidelity with the immersive first-person view. It addresses the commonly existing collision problems from previous method. For instance the red areas in (e). This can be visualized by comparing the sampled HMD views between Huang et al. [2016] (e) and ours (f)."
					}
				]
			},
			{
				"name": "Overview",
				"images": 
				[
					{
						"type":"Figure 2",
						"uri": "publications/2022-WallPlan/overview.jpg",
						"caption": "Overview of WallPlan. (a) The building boundary, as well as the front door (in bright yellow), is given as input of WallPlan. (b) The boundary is initialized with windows (in dark yellow) predicted by WinNet. (c) WallPlan generates both the wall graph and floorplan semantics by a coupled structure of GraphNet and LabelNet from the input boundary and windows. (d) The generated wall graph and floorplan semantics are used to construct a wall graph with room labels as output of WallPlan. The vectorized floorplan can be directly obtained from the wall graph. Note that the drawing of windows and interior doors which are generated using a heuristic method is only used for visualization, also adopted by other figures in this paper."
					}
				]
			},
			{
				"name": "Results",
				"images": 
				[
					{ 
						"type":"Figure 3",
						"uri": "publications/2022-WallPlan/result1.jpg",
						"caption": "Constrained floorplan generation. Top row: various design constraints applied to the same boundary. Middle row: output wall graphs. Botom row: floorplan visualization of wall graphs. (a) Only the input boundary. (b) Window constraint. (c) Load-bearing wall constraint (black dots and line segments). (d) Bubble diagram constraint (represented as the layout graph). (e) Hybrid-constraint. The overall structure of the floorplan changes significantly with diﬀerent constraints."
					},
					{
					    "type":"Figure 4",
						"uri": "publications/2022-WallPlan/result2.jpg",
						"caption": "Loading-bearing wall constrained generation. Top row: the input boundary as well as the loading-bearing walls. Middle row: the generated wall graphs with the loading-bearing walls (in black) using our method. Botom row: the corresponding floorplans for visualization."
					},
					{ 
						"type":"Figure 5",
						"uri": "publications/2022-WallPlan/result3.jpg",
						"caption": "Generating multiple floorplans from the same boundary with different constraints. Each column shows the different setings of the specific design constraint applied to the same boundary. (a) Window constraints. (b) Bubble diagram constraints. (c) Loading-bearing wall constraints."
					},
					{
						"type":"Figure 6",
						"uri": "publications/2022-WallPlan/result4.jpg",
						"caption": "Interpolation test. WallPlan generates floorplans from a set of interpolated boundaries. The first and last ones are the input boundaries for interpolation."
					}
				]
			}
		],
		"acknowlegement": "We would like to thank perceptual study participants for evaluating our system and the anonymous reviewers for their constructive suggestions and comments. This work is supported by the National Natural Science Foundation of China (62102126, 62025207, 61972128) and the Fundamental Research Funds for the Central Universities of China (PA2021KCPY0050).",
		"bibtex": 
		{
			"type": "article",
			"name": "sun2022wallplan",
			"title": "WallPlan: Synthesizing Floorplans by Learning to Generate Wall Graphs",
			"author": "Jiahui Sun and Wenming Wu and Ligang Liu and Wenjie Min and Gaofeng Zhang and Liping Zheng",
			"journel": "ACM Transactions on Graphics (SIGGRAPH)",
			"volume": "41",
			"number": "4",
			"pages": "1--14",
			"year": "2022"
		},
		"materials": 
		[ 
			{
				"iconUri": "publications/icons/pdf_wh64.png",
				"matUri": "publications/2022-WallPlan/paper.pdf",
				"caption": "Paper (~3MB)"
			},
			{
				"iconUri": "publications/icons/pdf_wh64.png",
				"matUri": "publications/2022-WallPlan/suppl.pdf",
				"caption": "Supplementary Material (~1MB)"
			},
			{
				"iconUri": "publications/icons/video_wh64.png",
				"matUri": "publications/2022-WallPlan/video.mp4",
				"caption": "Video (~65MB)"
			},
			{
				"iconUri": "publications/icons/ppt_wh64.png",
				"matUri": "publications/2022-WallPlan/slide.pptx",
				"caption": "Slide (~10MB)"
			},
			{
				"iconUri": "publications/icons/zip_wh64.png",
				"matUri": "publications/2022-WallPlan/code.zip",
				"caption": "Code (~1MB)"
			}
		]
	},
	{
		"id": 4,
		"title": "BubbleFormer: Bubble Diagram Generation via Dual Transformer Models",
		"authors": 
		[
			{
				"fullName": "Jiahui Sun",
				"url": "https://cgjiahui.github.io/"
			},
			{
				"fullName": "Liping Zheng",
				"url": "http://faculty.hfut.edu.cn/zhenglp/zh_CN/index.htm"
			},
			{
				"fullName": "Gaofeng Zhang",
				"url": "http://faculty.hfut.edu.cn/zhanggaofeng/zh_CN/index.htm"
			},
			{
				"fullName": "Wenming Wu",
				"url": "/"
			}
		],
		"affiliations": 
		[
			{
				"name": "Hefei University of Technology",
				"url": "http://www.hfut.edu.cn"
			}
		],
		"event": "Computer Graphics Forum (Proc. of Pacific Graphics 2023)",
		"abstract": "Bubble diagrams serve as a crucial tool in the feld of architectural planning and graphic design. With the surge of Artifcial Intelligence Generated Content (AIGC), there has been a continuous emergence of research and development efforts focused on utilizing bubble diagrams for layout design and generation. However, there is a lack of research efforts focused on bubble diagram generation. In this paper, we propose a novel generative model, BubbleFormer, for generating diverse and plausible bubble diagrams. BubbleFormer consists of two improved Transformer networks: NodeFormer and EdgeFormer. These networks generate nodes and edges of the bubble diagram, respectively. To enhance the generation diversity, a VAE module is incorporated into BubbleFormer, allowing for the sampling and generation of numerous high-quality bubble diagrams. BubbleFormer is trained end-to-end and evaluated through qualitative and quantitative experiments. The results demonstrate that BubbleFormer can generate convincing and diverse bubble diagrams, which in turn drive downstream tasks to produce high-quality layout plans. The model also shows generalization capabilities in other layout generation tasks and outperforms state-of-the-art techniques in terms of quality and diversity. In previous work, bubble diagrams as input are provided by users, and as a result, our bubble diagram generative model flls a signifcant gap in automated layout generation driven by bubble diagrams, thereby enabling an end-to-end layout design and generation. Code for this paper is at https://github.com/cgjiahui/BubbleFormer", 
		"contents": 
		[
			{
				"name": "Teaser",
				"images": 
				[
					{
						"type":"Figure 1",
						"uri": "publications/2023-BubbleFormer/teaser.jpg",
						"caption": "We propose BubbleFormer, a novel generative model for bubble diagram generation. BubbleFormer takes as input a layout boundary and outputs a large number of reasonable bubble diagrams constrained to the given boundary. These generated bubble diagrams can further be used as input to existing state-of-the-art generative models to produce high-quality layout plans."
					}
				]
			},
			{
				"name": "Overview",
				"images": 
				[
					{
						"type":"Figure 2",
						"uri": "publications/2023-BubbleFormer/overview.jpg",
						"caption": "Architecture of BubbleFormer. The network takes as input a building boundary as well as a sampled noise map from a standard normal distribution. The core module of BubbleFormer is dual Transformer models: NodeFormer and EdgeFormer, which are responsible for generating nodes and edges of the bubble diagram, respectively. The generated nodes and connections are matched and combined to get the fnal bubble diagram."
					}
				]
			},
			{
				"name": "Results",
				"images": 
				[
					{ 
						"type":"Figure 3",
						"uri": "publications/2023-BubbleFormer/result1.jpg",
						"caption": "Constrained generation. We illustrate the bubble diagrams generated with different boundaries and user constraints. The number of bedrooms, bathrooms, and balconies constrained are shown at the bottom."
					},
					{
					    "type":"Figure 4",
						"uri": "publications/2023-BubbleFormer/result2.jpg",
						"caption": "Unconstrained generation. We have tested the capability of BubbleFormer to generate bubble diagrams without any input."
					},
					{ 
						"type":"Figure 5",
						"uri": "publications/2023-BubbleFormer/result3.jpg",
						"caption": "Bubble diagram driven floor plan generation. Our method can further drive floor plan generation. Our generated bubble diagram can directly drive HouseGAN++ and WallPlan to obtain high-quality floor plans."
					},
					{
						"type":"Figure 6",
						"uri": "publications/2023-BubbleFormer/result4.jpg",
						"caption": "BubbleFormer can be applied to indoor furniture arrangemen and document layout generation.Top: indoor furniture arrangement of our method; Bottom: documnet layout generation of our method."
					},
					{ 
						"type":"Figure 7",
						"uri": "publications/2023-BubbleFormer/result5.jpg",
						"caption": "Interpolation test. BubbleFormer generates bubble diagrams from a set of interpolated noise maps. The first and last ones are generated with two different noise maps sampled from a standard normal distribution, and the intermediate results are interpolated using these two noise maps."
					}
				]
			}
		],
		"acknowlegement": "We would like to thank perceptual study participants for evaluating our system and the anonymous reviewers for their constructive suggestions and comments. This work is supported by the National Natural Science Foundation of China (62102126, 61972128) and the Fundamental Research Funds for the Central Universities of China (JZ2023HGTB0269, JZ2022HGQA0163).",
		"bibtex": 
		{
			"type": "article",
			"name": "sun2023BubbleFormer",
			"title": "Tailored Reality: Perception-aware Scene Restructuring for Adaptive VR Navigation",
			"author": "Jiahui Sun and Liping Zheng and Gaofeng Zhang and Wenming Wu",
			"journel": "Computer Graphics Forum (Pacific Graphics)",
			"volume": "42",
			"number": "7",
			"pages": "1--13",
			"year": "2023"
		},
		"materials": 
		[ 
			{
				"iconUri": "publications/icons/pdf_wh64.png",
				"matUri": "publications/2023-BubbleFormer/paper.pdf",
				"caption": "Paper (~4MB)"
			},
			{
				"iconUri": "publications/icons/pdf_wh64.png",
				"matUri": "publications/2023-BubbleFormer/suppl.pdf",
				"caption": "Supplementary Material (~0.5MB)"
			},
			{
				"iconUri": "publications/icons/video_wh64.png",
				"matUri": "publications/2023-BubbleFormer/video.mp4",
				"caption": "Video (~76MB)"
			},
			{
				"iconUri": "publications/icons/ppt_wh64.png",
				"matUri": "publications/2023-BubbleFormer/slide.pptx",
				"caption": "Slide (~13MB)"
			},
			{
				"iconUri": "publications/icons/dataset_wh64.png",
				"matUri": "publications/2023-BubbleFormer/dataset.zip",
				"caption": "Dataset"
			},
			{
				"iconUri": "publications/icons/zip_wh64.png",
				"matUri": "publications/2023-BubbleFormer/code.zip",
				"caption": "Code"
			}
		]
	},
	{
		"id": 5,
		"title": "Raster-to-Graph: Floorplan Recognition via Autoregressive Graph Prediction with an Attention Transformer",
		"authors":
		[
			{
				"fullName": "Sizhe Hu"
			},
			{
				"fullName": "Wenming Wu",
				"url": "/"
			},
			{
				"fullName": "Ruolin Su"
			},
			{
				"fullName": "Wanni Hou"
			},
			{
				"fullName": "Liping Zheng",
				"url": "http://faculty.hfut.edu.cn/zhenglp/zh_CN/index.htm"
			},
			{
				"fullName": "Benzhu Xu",
				"url": "https://faculty.hfut.edu.cn/xbz/zh_CN/index.htm"
			}
		],
		"affiliations":
		[
			{
				"name": "Hefei University of Technology",
				"url": "http://www.hfut.edu.cn"
			}
		],
		"event": "Computer Graphics Forum (Proc. of Eurographics 2024)",
		"abstract": "Recognizing the detailed information embedded in rasterized floorplans is at the research forefront in the community of computer graphics and vision. With the advent of deep neural networks, automatic floorplan recognition has made tremendous breakthroughs. However, co-recognizing both the structures and semantics of floorplans through one neural network remains a significant challenge. In this paper, we introduce a novel framework Raster-to-Graph, which automatically achieves structural and semantic recognition of floorplans. We represent vectorized floorplans as structural graphs embedded with floorplan semantics, thus transforming the floorplan recognition task into a structural graph prediction problem. We design an autoregressive prediction framework using the neural network architecture of the visual attention Transformer, iteratively predicting the wall junctions and wall segments of floorplans in the order of graph traversal. Additionally, we propose a large-scale floorplan dataset containing over 10,000 real-world residential floorplans. Our autoregressive framework can automatically recognize the structures and semantics of floorplans. Extensive experiments demonstrate the effectiveness of the proposed framework, showing significant improvements on all metrics. Qualitative and quantitative evaluations indicate that the proposed framework outperforms existing state-of-the-art methods. Code and dataset for this paper are available at: https://github.com/HSZVIS/Raster-to-Graph.", 
		"contents":
		[
			{
				"name": "Teaser",
				"images":
				[
					{
						"type":"Figure 1",
						"uri": "publications/2024-Raster2Graph/teaser.jpg",
						"caption": "We propose a novel floorplan vectorization framework for recognizing the semantics and structures of rasterized floorplans. Here shows a gallery of rasterized floorplans and the corresponding vectorized results obtained through our framework. For each group, from left to right: a given rasterized floorplan, a structural graph obtained by our method, a vectorized floorplan with various colors representing different categories of rooms and a popup 3D model of the floorplan."
					}
				]
			},
			{
				"name": "Overview",
				"images":
				[
					{
						"type":"Figure 2",
						"uri": "publications/2024-Raster2Graph/overview-dataset.jpg",
						"caption": "Overview of Dataset. (a1)-(a3): Statistics on the occurrence of wall junctions (a1), wall segments (a2), and rooms (a3). (b): Statistics on the occurrence of each room category. (c1)-(c3): A typical example in our dataset: rasterized floorplan image (c1), the corresponding structural graph (c2) and semantic floorplan (c3)."
					},
					{
						"type":"Figure 3",
						"uri": "publications/2024-Raster2Graph/overview.jpg",
						"caption": "Overview of Raster-to-Graph. We represent the vector floorplan as a structural graph with semantics, and then the task of floorplan recognition can be viewed as structural graph prediction. Our method takes the rasterized floorplan as input, iteratively predicts nodes (shown in yellow) and edges (shown in blue) of the structural graph in the order of graph traversal, and finally obtains a complete structural graph. A vectorized floorplan can be easily extracted using the structural graph. It’s worth noting that during the iterative process, our framework does not strictly adhere to the fixed order of graph traversal, as indicated in red. This is due to our training strategy, which allows a non-fixed order, a more general graph traversal, enhancing the flexibility of our prediction process."
					}
				]
			},
			{
				"name": "Results",
				"images":
				[
					{
						"type":"Figure 4",
						"uri": "publications/2024-Raster2Graph/result1.jpg",
						"caption": "Ablation study. From (b)-(d), only closed polygons are colored as rooms. In (e), we present the corresponding \"ground truth\" in our dataset. Note there might be a minor level of annotation errors in the semantic labeling of our dataset. For example, in Row 4, The Pipe-space has been incorrectly labeled as the Restroom."
					},
					{
						"type":"Figure 5",
						"uri": "publications/2024-Raster2Graph/result2.jpg",
						"caption": "Qualitative evaluation on structural reconstruction. From (b)-(e), all columns are shown in vectorized floorplans. In (e), we present the corresponding \"ground truth\" in our dataset."
					},
					{
						"type":"Figure 6",
						"uri": "publications/2024-Raster2Graph/result3.jpg",
						"caption": "Qualitative evaluation on semantic recognition. From (b)-(e), only closed polygons are colored as rooms. In (e), we present the corresponding \"ground truth\" in our dataset. Note there might be a minor level of annotation errors in the semantic labeling of our dataset."
					}
				]
			}
		],
		"acknowlegement": "We would like to thank the anonymous reviewers for their constructive\nsuggestions and comments. This work is supported by the National Natural Science Foundation of China (62102126, 62372153) and the Fundamental Research Funds for the Central Universities of China (JZ2023HGTB0269, JZ2022HGQA0163). In this paper, we used \"LIFULL HOME’S Dataset\" provided by LIFULL Co., Ltd. via IDR Dataset Service of National Institute of Informatics.",
		"bibtex":
		{
			"type": "article",
			"name": "hu2024RasterToGraph",
			"title": "Tailored Reality: Perception-aware Scene Restructuring for Adaptive VR Navigation",
			"author": "Sizhe Hu and Wenming Wu and Ruolin Su and Wanni Hou and Liping Zheng and Benzhu Xu",
			"journel": "Computer Graphics Forum (Eurographics)",
			"volume": "43",
			"number": "2",
			"pages": "1--14",
			"year": "2024"
		},
		"materials":
		[
			{
				"iconUri": "publications/icons/pdf_wh64.png",
				"matUri": "publications/2024-Raster2Graph/paper.pdf",
				"caption": "Paper (~6MB)"
			},
			{
				"iconUri": "publications/icons/video_wh64.png",
				"matUri": "publications/2024-Raster2Graph/video.mp4",
				"caption": "Video (~21MB)"
			},
			{
				"iconUri": "publications/icons/video_wh64.png",
				"matUri": "publications/2024-Raster2Graph/ff.mp4",
				"caption": "Fast Forward (~2MB)"
			},
			{
				"iconUri": "publications/icons/ppt_wh64.png",
				"matUri": "publications/2024-Raster2Graph/slide.pptx",
				"caption": "Slide (~78MB)"
			},
			{
				"iconUri": "publications/icons/dataset_wh64.png",
				"matUri": "publications/2024-Raster2Graph/dataset.zip",
				"caption": "Dataset (~0MB)"
			},
			{
				"iconUri": "publications/icons/zip_wh64.png",
				"matUri": "publications/2024-Raster2Graph/code.zip",
				"caption": "Code (~0MB)"
			}
		]
	},
	{   
		"id": 6,
		"title": "FuncScene: Function-centric Indoor Scene Synthesis via a Variational AutoEncoder Framework",
		"authors":
		[
			{
				"fullName": "Wenjie Min"
			},
			{
				"fullName": "Wenming Wu",
				"url": "/"
			},
			{
				"fullName": "Gaofeng Zhang",
				"url": "http://faculty.hfut.edu.cn/zhanggaofeng/zh_CN/index.htm"
			},
			{
				"fullName": "Liping Zheng",
				"url": "http://faculty.hfut.edu.cn/zhenglp/zh_CN/index.htm"
			}
		],
		"affiliations":
		[
			{
				"name": "Hefei University of Technology",
				"url": "http://www.hfut.edu.cn"
			}
		],
		"event": "Computer-Aided Geometric Design (Proc. of GMP 2024)",
		"abstract": "One of the main challenges of indoor scene synthesis is preserving the functionality of synthesized scenes to create practical and usable indoor environments. Function groups exhibit the capability of balancing the global structure and local scenes of an indoor space. In this paper, we propose a function-centric indoor scene synthesis framework, named FuncScene. Our key idea is to use function groups as an intermedium to connect the local scenes and the global structure, thus achieving a coarse-to-fne indoor scene synthesis while maintaining the functionality and practicality of synthesized scenes. Indoor scenes are synthesized by frst generating function groups using generative models and then instantiating by searching and matching the specifc function groups from a dataset. The proposed framework also makes it easier to achieve multi-level generation control of scene synthesis, which was challenging for previous works. Extensive experiments on various indoor scene synthesis tasks demonstrate the validity of our method. Qualitative and quantitative evaluations show the proposed framework outperforms the existing state-of-the-art.", 
		"contents":
		[
			{
				"name": "Teaser",
				"images":
				[
					{
						"type":"Figure 1",
						"uri": "publications/2024-FuncScene/teaser.jpg",
						"caption": "We introduce a novel model for generating indoor scenes, termed as FuncScene. Given a room boundary, FuncScene operates in two main steps. First, we generate function groups within the scene. Then, we instantiate these function groups by eﬀectively searching for and aligning them with relevant instances from an indoor scene dataset. This approach enables us to achieve enhanced control over the generation of indoor scene designs, ultimately leading to improved quality of the generated scenes. Top: input room boundaries for indoor scene synthesis. Middle: generated function groups for given boundaries. Bottom: fnal synthesised indoor scenes."
					}
				]
			},
			{
				"name": "Overview",
				"images":
				[
					{
						"type":"Figure 2",
						"uri": "publications/2024-FuncScene/overview.jpg",
						"caption": "Overview of FuncScene, which consists of three main parts. We frst acquire the number and categories of function groups of the scene through LabelVAE which predicts an ordered sequence of functional categories, and then BBoxVAE iteratively generates all necessary attributes of each function group based on the predicted functional category sequence. Finally, we instantiate the generated function groups by searching and matching them with corresponding instances from our indoor scene dataset."
					}
				]
			},
			{
				"name": "Results",
				"images":
				[
					{
						"type":"Figure 3",
						"uri": "publications/2024-FuncScene/result1.jpg",
						"caption": "Ablation Study. To evaluate the eﬀectiveness of our autoregressive per-attribute prediction, we have conducted an ablation study to simultaneously generate all attributes, as shown in (a). To evaluate the eﬀectiveness of our indoor scene synthesis in adaptive order, we have arranged the function groups within each scene in descending order of frequency and then made the model generate indoor scenes following this fxed order, as shown in (b). The results of our method are shown in (c)."
					},
					{
						"type":"Figure 4",
						"uri": "publications/2024-FuncScene/result2.jpg",
						"caption": "Diversity evaluation. We conduct the diversity evaluation of our method and baseline methods. From top to bottom, we show the generated indoor scenes by ATISS, the generated scenes by LayoutVAE, and the generated scenes by our method."
					},
					{
						"type":"Figure 5",
						"uri": "publications/2024-FuncScene/result3.jpg",
						"caption": "Plausibility evaluation. We conduct the plausibility evaluation of our method and baseline methods. From top to bottom, we show the indoor scenes of ground truth, the generated indoor scenes by ATISS, the generated scenes by LayoutVAE, and the generated scenes by our method."
					},
					{
						"type":"Figure 6",
						"uri": "publications/2024-FuncScene/result4.jpg",
						"caption": "Some results of quantitative evaluation. From top to bottom, it’s the generated scenes by ATISS, LayoutVAE, and our method. Our generated results exhibit better functional integrity than ATISS and LayoutVAE."
					},
					{
						"type":"Figure 7",
						"uri": "publications/2024-FuncScene/result5.jpg",
						"caption": "Scene completion. Given a partial scene, we attend automatically generate a complete indoor scene. From top to bottom, we show the indoor scenes of ground truth, the completed scene generated by ATISS, the completed scene generated by LayoutVAE, and the completed scene generated by our method."
					},
					{
						"type":"Figure 8",
						"uri": "publications/2024-FuncScene/result6.jpg",
						"caption": "Synthesis control. Our method can realize multi-level generation control for indoor scene generation, which means that we can realize more constraint control during indoor scene generation to satisfy multiple needs of users. From top to bottom, it’s the partial functionality constraint, complete functionality constraint, and a hybrid of complete functionality and partial scene constraint."
					}
				]
			}
		],
		"acknowlegement": "We would like to thank perceptual study participants for evaluating our system and the anonymous reviewers for their constructive suggestions and comments. This work is supported by the National Natural Science Foundation of China (62102126, 62372152), the National Key Research and Development Program of China (2022YFC3900800), and the Fundamental Research Funds for the Central Universities of China (JZ2023HGTB0269).",
		"bibtex":
		{
			"type": "article",
			"name": "min2024FuncScene",
			"title": "FuncScene: Function-centric Indoor Scene Synthesis via a Variational AutoEncoder Framework",
			"author": "Wenjie Min and Wenming Wu and Gaofeng Zhang and Liping Zheng",
			"journel": "Computer-Aided Geometric Design (GMP)",
			"volume": "111",
			"pages": "1--14",
			"year": "2024"
		},
		"materials":
		[
			{
				"iconUri": "publications/icons/pdf_wh64.png",
				"matUri": "publications/2024-FuncScene/paper.pdf",
				"caption": "Paper (~10MB)"
			},
			{
				"iconUri": "publications/icons/pdf_wh64.png",
				"matUri": "publications/2024-FuncScene/suppl.pdf",
				"caption": "Supplementary Material (~0.7MB)"
			},
			{
				"iconUri": "publications/icons/video_wh64.png",
				"matUri": "publications/2024-FuncScene/video.mp4",
				"caption": "Video (~10MB)"
			},
			{
				"iconUri": "publications/icons/ppt_wh64.png",
				"matUri": "publications/2024-FuncScene/slide.pptx",
				"caption": "Slide (~23MB)"
			}
		]
	}
]