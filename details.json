[
	{
		"title": "MIQP-based Layout Design for Building Interiors",
		"authors": 
		[
			{
				"fullName": "Wenming Wu",
				"url": "/",
				"afflID": [0, 1]
			},
			{
				"fullName": "Lubin Fan",
				"url": "https://lubinfan.github.io",
				"afflID": [1]
			},
			{
				"fullName": "Ligang Liu",
				"url": "http://staff.ustc.edu.cn/~lgliu",
				"afflID": [0]
			},
			{
				"fullName": "Peter Wonka",
				"url": "http://peterwonka.net",
				"afflID": [1]
			}
		],
		"affiliations": 
		[
			{
				"name": "University of Science and Technology of China",
				"url": "http://www.ustc.edu.cn"
			},
			{
				"name": "King Abdullah University of Science and Technology",
				"url": "http://www.kaust.edu.sa"
			}
		],
		"event": "Computer Grahpics Forum (Proc. of Eurographics 2018)",
		"abstract": "We propose a hierarchical framework for the generation of building interiors. Our solution is based on a mixed integer quadratic programming (MIQP) formulation. We parametrize a layout by polygons that are further decomposed into small rectangles. We identify important high-level constraints, such as room size, room position, room adjacency, and the outline of the building, and formulate them in a way that is compatible with MIQP and the problem parametrization. We also propose a hierarchical framework to improve the scalability of the approach. We demonstrate that our algorithm can be used for residential building layouts and can be scaled up to large layouts such as office buildings, shopping malls, and supermarkets. We show that our method is faster by multiple orders of magnitude than previous methods.",
		"contents": 
		[
			{
				"name": "Teaser",
				"images": 
				[
					{
						"type":"Figure 1",
						"uri": "publications/2018-IPLayout/teaser.png",
						"caption": "We propose a framework that generates building interiors with high-level constraints, e.g., room size, room position, room adjacency, and the outline of the building. (a) The layout of a two-storey house and corresponding 3D renderings. (b) A large-scale example depicting an office building. For such large-scale layouts, our method is faster by multiple orders of magnitude than previous methods."
					}
				]
			},
			{
				"name": "Overview",
				"images": 
				[
					{
						"type":"Figure 2",
						"uri": "publications/2018-IPLayout/overview.png",
						"caption": "Overview of our framework. Given a list of high-level constraints and an outline as input (Input), we compute a layout using a hierarchical framework. Level 1 shows an initial layout and Level 2 shows the layout including local refinements. Finally, additional details can be added to visualize the results as architectural floorplans or 3D models."
					}
				]
			},
			{
				"name": "Results",
				"images": 
				[
					{
					    "type":"Figure 3",
						"uri": "publications/2018-IPLayout/result1.png",
						"caption": "Two apartment layouts are generated by our algorithm with the same input. Boundary constraints are added for bedrooms to ensure that all bedrooms receive sufficient sunshine from the south east direction. The north direction is shown."
					},
					{ 
						"type":"Figure 4",
						"uri": "publications/2018-IPLayout/result2.png",
						"caption": "An interior layout of a bungalow-style house generated by our algorithm. The floorplan is shown in the middle. Two sides of the 3D rendering are shown."
					},
					{
						"type":"Figure 5",
						"uri": "publications/2018-IPLayout/result3.png",
						"caption": "An interior layout of a two-storey house. The stairs are considered as special rooms that should be consistent between two floors. The floorplan of each floor is shown on the left, 3D renderings are shown on the right."
					},
					{ 
						"type":"Figure 6",
						"uri": "publications/2018-IPLayout/result4.png",
						"caption": "Interior layouts of office buildings generated by our method. Polygons in the same color are from the same parent rectangle in a higher level."
					},
					{
						"type":"Figure 7",
						"uri": "publications/2018-IPLayout/result5.png",
						"caption": "Interior layouts of supermarkets generated by our method. Shelves for products in the same category are in the same color."
					}
				]
			}
		],
		"acknowlegement": "This work was supported by the KAUST Office of Sponsored Research (OSR) under Award No. OCRF-2014-CGR3-62140401, and the Visual Computing Center at KAUST. Ligang Liu is supported by the National Natural Science Foundation of China (61672482, 61672481, 11626253) and the One Hundred Talent Project of the Chinese Academy of Sciences. We would like to thank Virginia Unkefer for proofreading the paper.",
		"bibtex": 
		{
			"type": "article",
			"name": "wu2018miqp",
			"title": "MIQP-based Layout Design for Building Interiors",
			"author": "Wenming Wu and Lubin Fan and Ligang Liu and Peter Wonka",
			"journel": "Computer Grahpics Forum (SIGGRAPH Asia)",
			"volume": "37",
			"number": "2",
			"pages": "511--521",
			"year": "2018"
		},
		"materials": 
		[ 
			{
				"iconUri": "publications/icons/pdf_wh64.png",
				"matUri": "https://drive.google.com/file/d/1dgiRYRmBRnHzRu938kY7Krqu77dY-1LX/view?usp=share_link",
				"caption": "Paper (~108MB)"
			},
			{
				"iconUri": "publications/icons/pdf_wh64.png",
				"matUri": "publications/2018-IPLayout/paper_low_res.pdf",
				"caption": "Low-res Paper (~8MB)"
			},
			{
				"iconUri": "publications/icons/pdf_wh64.png",
				"matUri": "publications/2018-IPLayout/suppl.pdf",
				"caption": "Supplementary Material (~3MB)"
			},
			{
				"iconUri": "publications/icons/video_wh64.png",
				"matUri": "publications/2018-IPLayout/video.mp4",
				"caption": "Video (~50MB)"
			},
			{
				"iconUri": "publications/icons/ppt_wh64.png",
				"matUri": "publications/2018-IPLayout/slide.pptx",
				"caption": "Slide (~18MB)"
			}
		]
	},
	{
		"title": "Data-driven Interior Plan Generation for Residential Buildings",
		"authors": 
		[
			{
				"fullName": "Wenming Wu",
				"url": "/",
				"afflID": [0]
			},
			{
				"fullName": "Xiao-Ming Fu",
				"url": "https://ustc-gcl-f.github.io",
				"afflID": [0]
			},
			{
				"fullName": "Rui Tang",
				"afflID": [1]
			},
			{
				"fullName": "Yuhan Wang",
				"afflID": [1]
			},
			{
				"fullName": "Yu-Hao Qi",
				"afflID": [0]
			},
			{
				"fullName": "Ligang Liu",
				"url": "http://staff.ustc.edu.cn/~lgliu",
				"afflID": [0]
			}
		],
		"affiliations": 
		[
			{
				"name": "University of Science and Technology of China",
				"url": "http://www.ustc.edu.cn"
			},
			{
				"name": "Kujiale",
				"url": "https://www.kujiale.com"
			}
		],
		"event": "ACM Transactions on Graphics (Proc. of SIGGRAPH Asia 2019)",
		"abstract": "We propose a novel data-driven technique for automatically and efficiently generating floor plans for residential buildings with given boundaries. Central to this method is a two-stage approach that imitates the human design process by locating rooms first and then walls while adapting to the input building boundary. Based on observations of the presence of the living room in almost all fl oor plans, our designed learning network begins with positioning a living room and continues by iteratively generating other rooms. Then, walls are first determined by an encoder-decoder network, and then they are refined to vector representations using dedicated rules. To effectively train our networks, we construct RPLAN - a manually collected large-scale densely annotated dataset of floor plans from real residential buildings. Intensive experiments, including formative user studies and comparisons, are conducted to illustrate the feasibility and efficacy of our proposed approach. By comparing the plausibility of different floor plans, we have observed that our method substantially outperforms existing methods, and in many cases our floor plans are comparable to human-created ones.",
		"contents": 
		[
			{
				"name": "Teaser",
				"images": 
				[
					{
						"type":"Figure 1",
						"uri": "publications/2019-DeepLayout/teaser.png",
						"caption": "Our dataset RPLAN. (a) Statistics on the occurrence of each room type (a1), room number per floor plan (a2), the proportion of the area of the living room (a3), and the number of floor plan according to the total area (a4). (b) One typical floor plan in our collected dataset. (c) For each floor plan, we abstract the necessary information used in our method, including the boundary mask (the entrance is shown in red), inside mask, (interior) wall mask, and room mask."
					}
				]
			},
			{
				"name": "Overview",
				"images": 
				[
					{
						"type":"Figure 2",
						"uri": "publications/2019-DeepLayout/overview.png",
						"caption": "Overview of our method. Given an input boundary (a), our method first uses an iterative prediction model to obtain room locations (red dots in b). Based on the this, our method further locates walls (shown in blue) to obtain a vectorized floor plan (c). In this step, our method uses an encoder-decoder network to predict wall locations and then a post-processing step converts the floor plan into the final vector format. Additional details, including doors and windows, are added to visualize the floor plan (d)."
					}
				]
			},
			{
				"name": "Results",
				"images": 
				[
					{ 
						"type":"Figure 3",
						"uri": "publications/2019-DeepLayout/result1.png",
						"caption": "Examples of floor plans synthesized by our method, given one or two room locations specified by users (red dots). From lef to right: no specified room, specified master room, and specified kitchen and bathroom."
					},
					{
						"type":"Figure 4",
						"uri": "publications/2019-DeepLayout/result2.png",
						"caption": "Comparison to the nearest neighbors in the training dataset. Top row: the nearest neighbors. Botom row: our synthesized results."
					},
					{ 
						"type":"Figure 5",
						"uri": "publications/2019-DeepLayout/result3.png",
						"caption": "Floor plan generation for non axis-aligned boundaries. In the botom row, the examples contain curved walls."
					},
					{
						"type":"Figure 6",
						"uri": "publications/2019-DeepLayout/result4.png",
						"caption": "Synthesizing multiple floor plans given the same boundary as input."
					}
				]
			}
		],
		"acknowlegement": "We would like to thank Kai Wang for providing their implementation of [Wang et al. 2018], user study participants for evaluating our results, and the anonymous reviewers for their constructive suggestions and comments. This work is supported by the National Natural Science Foundation of China (61802359, 61672482, 11626253) and the Fundamental Research Funds for the Central Universities (WK0010460006, WK0010450004).",
		"bibtex": 
		{
			"type": "article",
			"name": "wu2019data",
			"title": "MIQP-based Layout Design for Building Interiors",
			"author": "Wenming Wu and Lubin Fan and Ligang Liu and Peter Wonka",
			"journel": "ACM Transactions on Graphics (SIGGRAPH Asia)",
			"volume": "38",
			"number": "6",
			"pages": "1--12",
			"year": "2019"
		},
		"materials": 
		[ 
			{
				"iconUri": "publications/icons/pdf_wh64.png",
				"matUri": "publications/2019-DeepLayout/paper.pdf",
				"caption": "Paper (~24MB)"
			},
			{
				"iconUri": "publications/icons/pdf_wh64.png",
				"matUri": "publications/2019-DeepLayout/suppl.pdf",
				"caption": "Supplementary Material (~7MB)"
			},
			{
				"iconUri": "publications/icons/video_wh64.png",
				"matUri": "publications/2019-DeepLayout/video.mp4",
				"caption": "Video (~12MB)"
			},
			{
				"iconUri": "publications/icons/ppt_wh64.png",
				"matUri": "publications/2019-DeepLayout/slide.pptx",
				"caption": "Slide (~10MB)"
			},
			{
				"iconUri": "publications/icons/dataset_wh64.png",
				"matUri": "https://docs.google.com/forms/d/e/1FAIpQLSfwteilXzURRKDI5QopWCyOGkeb_CFFbRwtQ0SOPhEg0KGSfw/viewform?usp=sf_link",
				"caption": "Dataset"
			},
			{
				"iconUri": "publications/icons/zip_wh64.png",
				"matUri": "https://drive.google.com/file/d/1bMURobr16oeD7wT8jbVuUIJPgOTtOYWs/view?usp=share_link",
				"caption": "Code (~155MB)"
			}
		]
	},
	{
		"title": "Tailored Reality: Perception-Aware Scene Restructuring for Adaptive VR Navigation",
		"authors": 
		[
			{
				"fullName": "Zhi-Chao Dong",
				"afflID": [0]
			},
			{
				"fullName": "Wenming Wu",
				"url": "/",
				"afflID": [0]
			},
			{
				"fullName": "Zeng-Hao Xu",
				"afflID": [0]
			},
			{
				"fullName": "Qi Sun",
				"url": "https://qisun.me",
				"afflID": [1]
			},
			{
				"fullName": "Guan-Jie Yuan",
				"afflID": [0]
			},
			{
				"fullName": "Ligang Liu",
				"url": "http://staff.ustc.edu.cn/~lgliu",
				"afflID": [0]
			},
			{
				"fullName": "Xiao-Ming Fu",
				"url": "https://ustc-gcl-f.github.io",
				"afflID": [0]
			}
		],
		"affiliations": 
		[
			{
				"name": "University of Science and Technology of China",
				"url": "http://www.ustc.edu.cn"
			},
			{
				"name": "New York University",
				"url": "https://www.nyu.edu"
			}
		],
		"event": "ACM Transactions on Graphics (Proc. of SIGGRAPH Asia 2019)",
		"abstract": "In virtual reality (VR), the virtual scenes are pre-designed by creators. Our physical surroundings, however, comprise signifcantly varied sizes, layouts, and components. To bridge the gap and further enable natural navigation, recent solutions have been proposed to redirect users or recreate the virtual content. However, they suffer from either interrupted experience or distorted appearances. We present a novel VR-oriented algorithm that automatically restructures a given virtual scene for a user’s physical environment. Different from the previous methods, we introduce neither interrupted walking experience nor curved appearances. Instead, a perceptionaware function optimizes our retargeting technique to preserve the fdelity of the virtual scene that appears in VR head-mounted displays. Besides geometric and topological properties, it emphasizes the unique first-person view perceptual factors in VR, such as dynamic visibility and objectwise relationships. We conduct both analytical experiments and subjective studies. The results demonstrate our system’s versatile capability and practicability for natural navigation in VR: It reduces the virtual space by 40% without statistical loss of perceptual identicality.", 
		"contents": 
		[
			{
				"name": "Teaser",
				"images": 
				[
					{
						"type":"Figure 1",
						"uri": "publications/2021-TailoredReality/teaser.png",
						"caption": "Illustration of our system. Tailored for first-person view VR applications, we propose a novel perception-aware scene retargeting method. It restructures a given virtual scene (a) for users’ small and varied physical spaces (valid area in (b) and (c)). Our technique preserves not only geometric and topological properties but also perceptual fidelity with the immersive first-person view. It addresses the commonly existing collision problems from previous method. For instance the red areas in (e). This can be visualized by comparing the sampled HMD views between Huang et al. [2016] (e) and ours (f)."
					}
				]
			},
			{
				"name": "Results",
				"images": 
				[
					{ 
						"type":"Figure 2",
						"uri": "publications/2021-TailoredReality/result1.png",
						"caption": "The original indoor scene and the retargeted result using our method. Top: Floor plans of the original indoor scene and our result. Middle: several sampling HMD views from the original indoor scene. Botom: Corresponding views from our result."
					},
					{
					    "type":"Figure 3",
						"uri": "publications/2021-TailoredReality/result2.png",
						"caption": "Various numbers of salient objects. From lef to right, the number of salient objects in the virtual scene decreases monotonously."
					},
					{ 
						"type":"Figure 4",
						"uri": "publications/2021-TailoredReality/result3.png",
						"caption": "Different real spaces with the same area. The values above the retargeted scene indicate the ratios between the width and height of the real space and the input virtual scene."
					},
					{
						"type":"Figure 5",
						"uri": "publications/2021-TailoredReality/result4.png",
						"caption": "Retargeting results of different sizes from 0.75 to 0.45. The first one is shown as the origin scene. Corresponding HMD views at the same position are also shown for each scene. As the retargeting size decreases, the visual distortion of the scene becomes more and more pronounced. The object-level distortion along with the compression stress level is ploted in Figure 20. The objective metrics of the retargeting results are shown in then supplementary materials."
					}
				]
			}
		],
		"acknowlegement": "We thank user study participants for evaluating our system and the anonymous reviewers for their constructive suggestions and comments. This work is supported by the National Natural Science Foundation of China (61802359 and 62025207), Zhejiang Lab (NO.2019NB0AB03), and USTC Research Funds of Double First-Class Initiative (YD0010002003).",
		"bibtex": 
		{
			"type": "article",
			"name": "dong2021tailored",
			"title": "Tailored Reality: Perception-aware Scene Restructuring for Adaptive VR Navigation",
			"author": "Zhi-Chao Dong and Wenming Wu and Zeng-Hao Xu and Qi Sun and Guan-Jie Yuan and Ligang Liu and Xiao-Ming Fu",
			"journel": "ACM Transactions on Graphics",
			"volume": "40",
			"number": "5",
			"pages": "1--15",
			"year": "2021"
		},
		"materials": 
		[ 
			{
				"iconUri": "publications/icons/pdf_wh64.png",
				"matUri": "publications/2021-TailoredReality/paper.pdf",
				"caption": "Paper (~22MB)"
			},
			{
				"iconUri": "publications/icons/video_wh64.png",
				"matUri": "publications/2021-TailoredReality/video.mp4",
				"caption": "Video (~52MB)"
			},
			{
				"iconUri": "publications/icons/ppt_wh64.png",
				"matUri": "https://docs.google.com/presentation/d/1KJ4_pZ240w8eBiMDjDVb7t1ceOjinkJd/edit?usp=share_link&ouid=104838625643552629233&rtpof=true&sd=true",
				"caption": "Slide (~343MB)"
			},
			{
				"iconUri": "publications/icons/zip_wh64.png",
				"matUri": "publications/2021-TailoredReality/code.zip",
				"caption": "Code (~20MB)"
			}
		]
	},
	{
		"title": "WallPlan: Synthesizing Floorplans by Learning to Generate Wall Graphs",
		"authors": 
		[
			{
				"fullName": "Jiahui Sun",
				"afflID": [0]
			},
			{
				"fullName": "Wenming Wu",
				"url": "/",
				"afflID": [0]
			},
			{
				"fullName": "Ligang Liu",
				"url": "http://staff.ustc.edu.cn/~lgliu",
				"afflID": [1]
			},
			{
				"fullName": "Wenjie Min",
				"afflID": [0]
			},
			{
				"fullName": "Gaofeng Zhang",
				"afflID": [0]
			},
			{
				"fullName": "Liping Zheng",
				"afflID": [0]
			}
		],
		"affiliations": 
		[
			{
				"name": "Hefei University of Technology",
				"url": "https://www.hfut.edu.cn"
			},
			{
				"name": "University of Science and Technology of China",
				"url": "http://www.ustc.edu.cn"
			}
		],
		"event": "ACM Transactions on Graphics (Proc. of SIGGRAPH 2022)",
		"abstract": "Floorplan generation has drawn widespread interest in the community. Recent learning-based methods for generating realistic floorplans have made signifcant progress while a complex heuristic post-processing is still necessary to obtain desired results. In this paper, we propose a novel wall-oriented method, called WallPlan, for automatically and efciently generating plausible floorplans from various design constraints. We pioneer the representation of the floorplan as a wall graph with room labels and consider the floorplan generation as a graph generation. Given the boundary as input, we first initialize the boundary with windows predicted by WinNet. Then a graph generation network GraphNet and semantics prediction network LabelNet are coupled to generate the wall graph progressively by imitating graph traversal. WallPlan can be applied for practical architectural designs, especially the wall-based constraints. We conduct ablation experiments, qualitative evaluations, quantitative comparisons, and perceptual studies to evaluate our method’s feasibility, efcacy, and versatility. Intensive experiments demonstrate our method requires no post-processing, producing higher quality floorplans than state-of-the-art techniques.", 
		"contents": 
		[
			{
				"name": "Teaser",
				"images": 
				[
					{
						"type":"Figure 1",
						"uri": "publications/2022-WallPlan/teaser.png",
						"caption": "Illustration of our system. Tailored for first-person view VR applications, we propose a novel perception-aware scene retargeting method. It restructures a given virtual scene (a) for users’ small and varied physical spaces (valid area in (b) and (c)). Our technique preserves not only geometric and topological properties but also perceptual fidelity with the immersive first-person view. It addresses the commonly existing collision problems from previous method. For instance the red areas in (e). This can be visualized by comparing the sampled HMD views between Huang et al. [2016] (e) and ours (f)."
					}
				]
			},
			{
				"name": "Overview",
				"images": 
				[
					{
						"type":"Figure 2",
						"uri": "publications/2022-WallPlan/overview.png",
						"caption": "Overview of WallPlan. (a) The building boundary, as well as the front door (in bright yellow), is given as input of WallPlan. (b) The boundary is initialized with windows (in dark yellow) predicted by WinNet. (c) WallPlan generates both the wall graph and floorplan semantics by a coupled structure of GraphNet and LabelNet from the input boundary and windows. (d) The generated wall graph and floorplan semantics are used to construct a wall graph with room labels as output of WallPlan. The vectorized floorplan can be directly obtained from the wall graph. Note that the drawing of windows and interior doors which are generated using a heuristic method is only used for visualization, also adopted by other figures in this paper."
					}
				]
			},
			{
				"name": "Results",
				"images": 
				[
					{ 
						"type":"Figure 3",
						"uri": "publications/2022-WallPlan/result1.png",
						"caption": "Constrained floorplan generation. Top row: various design constraints applied to the same boundary. Middle row: output wall graphs. Botom row: floorplan visualization of wall graphs. (a) Only the input boundary. (b) Window constraint. (c) Load-bearing wall constraint (black dots and line segments). (d) Bubble diagram constraint (represented as the layout graph). (e) Hybrid-constraint. The overall structure of the floorplan changes significantly with diﬀerent constraints."
					},
					{
					    "type":"Figure 4",
						"uri": "publications/2022-WallPlan/result2.png",
						"caption": "Loading-bearing wall constrained generation. Top row: the input boundary as well as the loading-bearing walls. Middle row: the generated wall graphs with the loading-bearing walls (in black) using our method. Botom row: the corresponding floorplans for visualization."
					},
					{ 
						"type":"Figure 5",
						"uri": "publications/2022-WallPlan/result3.png",
						"caption": "Generating multiple floorplans from the same boundary with different constraints. Each column shows the different setings of the specific design constraint applied to the same boundary. (a) Window constraints. (b) Bubble diagram constraints. (c) Loading-bearing wall constraints."
					},
					{
						"type":"Figure 6",
						"uri": "publications/2022-WallPlan/result4.png",
						"caption": "Interpolation test. WallPlan generates floorplans from a set of interpolated boundaries. The first and last ones are the input boundaries for interpolation."
					}
				]
			}
		],
		"acknowlegement": "We would like to thank perceptual study participants for evaluating our system and the anonymous reviewers for their constructive suggestions and comments. This work is supported by the National Natural Science Foundation of China (62102126, 62025207, 61972128) and the Fundamental Research Funds for the Central Universities of China (PA2021KCPY0050).",
		"bibtex": 
		{
			"type": "article",
			"name": "sun2022wallplan",
			"title": "WallPlan: Synthesizing Floorplans by Learning to Generate Wall Graphs",
			"author": "Jiahui Sun and Wenming Wu and Ligang Liu and Wenjie Min and Gaofeng Zhang and Liping Zheng",
			"journel": "ACM Transactions on Graphics (SIGGRAPH)",
			"volume": "41",
			"number": "4",
			"pages": "1--14",
			"year": "2022"
		},
		"materials": 
		[ 
			{
				"iconUri": "publications/icons/pdf_wh64.png",
				"matUri": "publications/2022-WallPlan/paper.pdf",
				"caption": "Paper (~3MB)"
			},
			{
				"iconUri": "publications/icons/pdf_wh64.png",
				"matUri": "publications/2022-WallPlan/suppl.pdf",
				"caption": "Supplementary Material (~1MB)"
			},
			{
				"iconUri": "publications/icons/video_wh64.png",
				"matUri": "publications/2022-WallPlan/video.mp4",
				"caption": "Video (~65MB)"
			},
			{
				"iconUri": "publications/icons/ppt_wh64.png",
				"matUri": "publications/2022-WallPlan/slide.pptx",
				"caption": "Slide (~10MB)"
			},
			{
				"iconUri": "publications/icons/zip_wh64.png",
				"matUri": "publications/2022-WallPlan/code.zip",
				"caption": "Code (~1MB)"
			}
		]
	},
	{
		"title": "BubbleFormer: Bubble Diagram Generation via Dual Transformer Models",
		"authors": 
		[
			{
				"fullName": "Jiahui Sun"
			},
			{
				"fullName": "Liping Zheng"
			},
			{
				"fullName": "Gaofeng Zhang"
			},
			{
				"fullName": "Wenming Wu",
				"url": "/"
			}
		],
		"affiliations": 
		[
			{
				"name": "Hefei University of Technology",
				"url": "http://www.hfut.edu.cn"
			}
		],
		"event": "Computer Graphics Forum (Proc. of Pacific Graphics 2023)",
		"abstract": "Bubble diagrams serve as a crucial tool in the feld of architectural planning and graphic design. With the surge of Artifcial Intelligence Generated Content (AIGC), there has been a continuous emergence of research and development efforts focused on utilizing bubble diagrams for layout design and generation. However, there is a lack of research efforts focused on bubble diagram generation. In this paper, we propose a novel generative model, BubbleFormer, for generating diverse and plausible bubble diagrams. BubbleFormer consists of two improved Transformer networks: NodeFormer and EdgeFormer. These networks generate nodes and edges of the bubble diagram, respectively. To enhance the generation diversity, a VAE module is incorporated into BubbleFormer, allowing for the sampling and generation of numerous high-quality bubble diagrams. BubbleFormer is trained end-to-end and evaluated through qualitative and quantitative experiments. The results demonstrate that BubbleFormer can generate convincing and diverse bubble diagrams, which in turn drive downstream tasks to produce high-quality layout plans. The model also shows generalization capabilities in other layout generation tasks and outperforms state-of-the-art techniques in terms of quality and diversity. In previous work, bubble diagrams as input are provided by users, and as a result, our bubble diagram generative model flls a signifcant gap in automated layout generation driven by bubble diagrams, thereby enabling an end-to-end layout design and generation. Code for this paper is at https://github.com/cgjiahui/BubbleFormer", 
		"contents": 
		[
			{
				"name": "Teaser",
				"images": 
				[
					{
						"type":"Figure 1",
						"uri": "publications/2023-BubbleFormer/teaser.png",
						"caption": "We propose BubbleFormer, a novel generative model for bubble diagram generation. BubbleFormer takes as input a layout boundary and outputs a large number of reasonable bubble diagrams constrained to the given boundary. These generated bubble diagrams can further be used as input to existing state-of-the-art generative models to produce high-quality layout plans."
					}
				]
			},
			{
				"name": "Overview",
				"images": 
				[
					{
						"type":"Figure 2",
						"uri": "publications/2023-BubbleFormer/overview.png",
						"caption": "Architecture of BubbleFormer. The network takes as input a building boundary as well as a sampled noise map from a standard normal distribution. The core module of BubbleFormer is dual Transformer models: NodeFormer and EdgeFormer, which are responsible for generating nodes and edges of the bubble diagram, respectively. The generated nodes and connections are matched and combined to get the fnal bubble diagram."
					}
				]
			},
			{
				"name": "Results",
				"images": 
				[
					{ 
						"type":"Figure 3",
						"uri": "publications/2023-BubbleFormer/result1.png",
						"caption": "Constrained generation. We illustrate the bubble diagrams generated with different boundaries and user constraints. The number of bedrooms, bathrooms, and balconies constrained are shown at the bottom."
					},
					{
					    "type":"Figure 4",
						"uri": "publications/2023-BubbleFormer/result2.png",
						"caption": "Unconstrained generation. We have tested the capability of BubbleFormer to generate bubble diagrams without any input."
					},
					{ 
						"type":"Figure 5",
						"uri": "publications/2023-BubbleFormer/result3.png",
						"caption": "Bubble diagram driven floor plan generation. Our method can further drive floor plan generation. Our generated bubble diagram can directly drive HouseGAN++ and WallPlan to obtain high-quality floor plans."
					},
					{
						"type":"Figure 6",
						"uri": "publications/2023-BubbleFormer/result4.png",
						"caption": "BubbleFormer can be applied to indoor furniture arrangemen and document layout generation.Top: indoor furniture arrangement of our method; Bottom: documnet layout generation of our method."
					},
					{ 
						"type":"Figure 7",
						"uri": "publications/2023-BubbleFormer/result5.png",
						"caption": "Interpolation test. BubbleFormer generates bubble diagrams from a set of interpolated noise maps. The first and last ones are generated with two different noise maps sampled from a standard normal distribution, and the intermediate results are interpolated using these two noise maps."
					}
				]
			}
		],
		"acknowlegement": "We would like to thank perceptual study participants for evaluating our system and the anonymous reviewers for their constructive suggestions and comments. This work is supported by the National Natural Science Foundation of China (62102126, 61972128) and the Fundamental Research Funds for the Central Universities of China (JZ2023HGTB0269, JZ2022HGQA0163).",
		"bibtex": 
		{
			"type": "article",
			"name": "sun2023BubbleFormer",
			"title": "Tailored Reality: Perception-aware Scene Restructuring for Adaptive VR Navigation",
			"author": "Jiahui Sun and Liping Zheng and Gaofeng Zhang and Wenming Wu",
			"journel": "Computer Graphics Forum (Pacific Graphics)",
			"volume": "42",
			"number": "7",
			"pages": "1--13",
			"year": "2023"
		},
		"materials": 
		[ 
			{
				"iconUri": "publications/icons/pdf_wh64.png",
				"matUri": "publications/2023-BubbleFormer/paper.pdf",
				"caption": "Paper (~4MB)"
			},
			{
				"iconUri": "publications/icons/pdf_wh64.png",
				"matUri": "publications/2023-BubbleFormer/suppl.pdf",
				"caption": "Supplementary Material (~0.5MB)"
			},
			{
				"iconUri": "publications/icons/video_wh64.png",
				"matUri": "publications/2023-BubbleFormer/video.mp4",
				"caption": "Video (~76MB)"
			},
			{
				"iconUri": "publications/icons/ppt_wh64.png",
				"matUri": "publications/2023-BubbleFormer/slide.pptx",
				"caption": "Slide (~13MB)"
			},
			{
				"iconUri": "publications/icons/zip_wh64.png",
				"matUri": "/",
				"caption": "Code"
			}
		]
	}
]